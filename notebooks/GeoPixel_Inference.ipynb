{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "GeoPixel-Inference",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anarghya15/notebooks/blob/main/notebooks/GeoPixel_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mbzuai-oryx/GeoPixel.git"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-10T09:06:31.812361Z",
          "iopub.execute_input": "2025-03-10T09:06:31.81263Z",
          "iopub.status.idle": "2025-03-10T09:06:33.495391Z",
          "shell.execute_reply.started": "2025-03-10T09:06:31.812602Z",
          "shell.execute_reply": "2025-03-10T09:06:33.494145Z"
        },
        "id": "f4o9ovCLjq8p",
        "outputId": "6efa6828-bc72-472c-c5f0-9844a962d4eb"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Cloning into 'GeoPixel'...\nremote: Enumerating objects: 357, done.\u001b[K\nremote: Counting objects: 100% (118/118), done.\u001b[K\nremote: Compressing objects: 100% (117/117), done.\u001b[K\nremote: Total 357 (delta 65), reused 0 (delta 0), pack-reused 239 (from 2)\u001b[K\nReceiving objects: 100% (357/357), 30.31 MiB | 39.19 MiB/s, done.\nResolving deltas: 100% (140/140), done.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /kaggle/working/GeoPixel"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-10T09:06:33.496733Z",
          "iopub.execute_input": "2025-03-10T09:06:33.497045Z",
          "iopub.status.idle": "2025-03-10T09:06:33.503908Z",
          "shell.execute_reply.started": "2025-03-10T09:06:33.497016Z",
          "shell.execute_reply": "2025-03-10T09:06:33.503036Z"
        },
        "id": "MWVqXX85jq8r",
        "outputId": "33508b31-2039-4a67-8115-ed7a62f59385"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "/kaggle/working/GeoPixel\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install flash-attn==2.6.3 --no-build-isolation"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-10T09:06:33.504968Z",
          "iopub.execute_input": "2025-03-10T09:06:33.505275Z",
          "iopub.status.idle": "2025-03-10T09:09:30.545745Z",
          "shell.execute_reply.started": "2025-03-10T09:06:33.505246Z",
          "shell.execute_reply": "2025-03-10T09:09:30.544574Z"
        },
        "id": "vnrk4S-Kjq8s",
        "outputId": "c3124848-4a8c-4685-bd13-003c1f599ab6"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Looking in indexes: https://download.pytorch.org/whl/cu121\nCollecting torch==2.3.1\n  Downloading https://download.pytorch.org/whl/cu121/torch-2.3.1%2Bcu121-cp310-cp310-linux_x86_64.whl (781.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.0/781.0 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchvision==0.18.1\n  Downloading https://download.pytorch.org/whl/cu121/torchvision-0.18.1%2Bcu121-cp310-cp310-linux_x86_64.whl (7.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting torchaudio==2.3.1\n  Downloading https://download.pytorch.org/whl/cu121/torchaudio-2.3.1%2Bcu121-cp310-cp310-linux_x86_64.whl (3.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m75.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (2024.12.0)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.1)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.1)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.1)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.1)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.1)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.1)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.1)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.1)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.1)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.1)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.1)\n  Downloading https://download.pytorch.org/whl/cu121/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting triton==2.3.1 (from torch==2.3.1)\n  Downloading https://download.pytorch.org/whl/triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.1) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision==0.18.1) (11.0.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1) (12.6.85)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.1) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.18.1) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.18.1) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.18.1) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.18.1) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.18.1) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision==0.18.1) (2.4.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.1) (1.3.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision==0.18.1) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision==0.18.1) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision==0.18.1) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision==0.18.1) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision==0.18.1) (2024.2.0)\nInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, torchaudio, torchvision\n  Attempting uninstall: nvidia-nccl-cu12\n    Found existing installation: nvidia-nccl-cu12 2.23.4\n    Uninstalling nvidia-nccl-cu12-2.23.4:\n      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.7.77\n    Uninstalling nvidia-curand-cu12-10.3.7.77:\n      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.6.0.74\n    Uninstalling nvidia-cudnn-cu12-9.6.0.74:\n      Successfully uninstalled nvidia-cudnn-cu12-9.6.0.74\n  Attempting uninstall: torch\n    Found existing installation: torch 2.5.1+cu121\n    Uninstalling torch-2.5.1+cu121:\n      Successfully uninstalled torch-2.5.1+cu121\n  Attempting uninstall: torchaudio\n    Found existing installation: torchaudio 2.5.1+cu121\n    Uninstalling torchaudio-2.5.1+cu121:\n      Successfully uninstalled torchaudio-2.5.1+cu121\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.20.1+cu121\n    Uninstalling torchvision-0.20.1+cu121:\n      Successfully uninstalled torchvision-0.20.1+cu121\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\npylibcugraph-cu12 24.10.0 requires pylibraft-cu12==24.10.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.10.0 requires rmm-cu12==24.10.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.3.1+cu121 torchaudio-2.3.1+cu121 torchvision-0.18.1+cu121 triton-2.3.1\nCollecting flash-attn==2.6.3\n  Downloading flash_attn-2.6.3.tar.gz (2.6 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn==2.6.3) (2.3.1+cu121)\nRequirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn==2.6.3) (0.8.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.6.3) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.6.3) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.6.3) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.6.3) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.6.3) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.6.3) (2024.12.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.6.3) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.6.3) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.6.3) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.6.3) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.6.3) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.6.3) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.6.3) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.6.3) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.6.3) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.6.3) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.6.3) (12.1.105)\nRequirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.6.3) (2.3.1)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->flash-attn==2.6.3) (12.6.85)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn==2.6.3) (3.0.2)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->flash-attn==2.6.3) (1.3.0)\nBuilding wheels for collected packages: flash-attn\n  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for flash-attn: filename=flash_attn-2.6.3-cp310-cp310-linux_x86_64.whl size=187290390 sha256=c50f5de67a8b75bcfcf4a34257622475f9b56d59da58a539476a21db9d5b9867\n  Stored in directory: /root/.cache/pip/wheels/7e/e3/c3/89c7a2f3c4adc07cd1c675f8bb7b9ad4d18f64a72bccdfe826\nSuccessfully built flash-attn\nInstalling collected packages: flash-attn\nSuccessfully installed flash-attn-2.6.3\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -r requirements.txt"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-10T09:09:30.546926Z",
          "iopub.execute_input": "2025-03-10T09:09:30.547247Z",
          "iopub.status.idle": "2025-03-10T09:10:12.64318Z",
          "shell.execute_reply.started": "2025-03-10T09:09:30.547222Z",
          "shell.execute_reply": "2025-03-10T09:10:12.642076Z"
        },
        "id": "k1LCyqudjq8t",
        "outputId": "2a3058cb-087d-42be-d6cd-26b92951491e"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting accelerate==0.34.2 (from -r requirements.txt (line 1))\n  Downloading accelerate-0.34.2-py3-none-any.whl.metadata (19 kB)\nCollecting decord==0.6.0 (from -r requirements.txt (line 2))\n  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\nCollecting deepspeed==0.12.3 (from -r requirements.txt (line 3))\n  Downloading deepspeed-0.12.3.tar.gz (1.2 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: einops==0.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (0.8.0)\nCollecting hydra-core==1.3.2 (from -r requirements.txt (line 5))\n  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\nCollecting matplotlib==3.9.2 (from -r requirements.txt (line 6))\n  Downloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.26.4)\nRequirement already satisfied: opencv-python==4.10.0.84 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (4.10.0.84)\nCollecting peft==0.8.2 (from -r requirements.txt (line 9))\n  Downloading peft-0.8.2-py3-none-any.whl.metadata (25 kB)\nRequirement already satisfied: pycocotools==2.0.8 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (2.0.8)\nCollecting sentencepiece==0.1.99 (from -r requirements.txt (line 11))\n  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\nCollecting tensorboard==2.18.0 (from -r requirements.txt (line 12))\n  Downloading tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\nCollecting transformers==4.33.2 (from -r requirements.txt (line 13))\n  Downloading transformers-4.33.2-py3-none-any.whl.metadata (119 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.9/119.9 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2->-r requirements.txt (line 1)) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2->-r requirements.txt (line 1)) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2->-r requirements.txt (line 1)) (6.0.2)\nRequirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2->-r requirements.txt (line 1)) (2.3.1+cu121)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2->-r requirements.txt (line 1)) (0.29.0)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2->-r requirements.txt (line 1)) (0.4.5)\nCollecting hjson (from deepspeed==0.12.3->-r requirements.txt (line 3))\n  Downloading hjson-3.1.0-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: ninja in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.12.3->-r requirements.txt (line 3)) (1.11.1.3)\nRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.12.3->-r requirements.txt (line 3)) (9.0.0)\nRequirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.12.3->-r requirements.txt (line 3)) (2.11.0a2)\nRequirement already satisfied: pynvml in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.12.3->-r requirements.txt (line 3)) (12.0.0)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.12.3->-r requirements.txt (line 3)) (4.67.1)\nRequirement already satisfied: omegaconf<2.4,>=2.2 in /usr/local/lib/python3.10/dist-packages (from hydra-core==1.3.2->-r requirements.txt (line 5)) (2.3.0)\nRequirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from hydra-core==1.3.2->-r requirements.txt (line 5)) (4.9.3)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.2->-r requirements.txt (line 6)) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.2->-r requirements.txt (line 6)) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.2->-r requirements.txt (line 6)) (4.55.3)\nRequirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.2->-r requirements.txt (line 6)) (1.4.7)\nRequirement already satisfied: pillow>=8 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.2->-r requirements.txt (line 6)) (11.0.0)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.2->-r requirements.txt (line 6)) (3.2.0)\nRequirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib==3.9.2->-r requirements.txt (line 6)) (2.9.0.post0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->-r requirements.txt (line 7)) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->-r requirements.txt (line 7)) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->-r requirements.txt (line 7)) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->-r requirements.txt (line 7)) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->-r requirements.txt (line 7)) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy==1.26.4->-r requirements.txt (line 7)) (2.4.1)\nRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.18.0->-r requirements.txt (line 12)) (1.4.0)\nRequirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.18.0->-r requirements.txt (line 12)) (1.68.1)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.18.0->-r requirements.txt (line 12)) (3.7)\nRequirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.18.0->-r requirements.txt (line 12)) (3.20.3)\nRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.18.0->-r requirements.txt (line 12)) (75.1.0)\nRequirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.18.0->-r requirements.txt (line 12)) (1.17.0)\nRequirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.18.0->-r requirements.txt (line 12)) (0.7.2)\nRequirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard==2.18.0->-r requirements.txt (line 12)) (3.1.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2->-r requirements.txt (line 13)) (3.17.0)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2->-r requirements.txt (line 13)) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.33.2->-r requirements.txt (line 13)) (2.32.3)\nCollecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.33.2->-r requirements.txt (line 13))\n  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate==0.34.2->-r requirements.txt (line 1)) (2024.12.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate==0.34.2->-r requirements.txt (line 1)) (4.12.2)\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2->-r requirements.txt (line 1)) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2->-r requirements.txt (line 1)) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2->-r requirements.txt (line 1)) (3.1.4)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2->-r requirements.txt (line 1)) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2->-r requirements.txt (line 1)) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2->-r requirements.txt (line 1)) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2->-r requirements.txt (line 1)) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2->-r requirements.txt (line 1)) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2->-r requirements.txt (line 1)) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2->-r requirements.txt (line 1)) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2->-r requirements.txt (line 1)) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2->-r requirements.txt (line 1)) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2->-r requirements.txt (line 1)) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2->-r requirements.txt (line 1)) (12.1.105)\nRequirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.34.2->-r requirements.txt (line 1)) (2.3.1)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.34.2->-r requirements.txt (line 1)) (12.6.85)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard==2.18.0->-r requirements.txt (line 12)) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy==1.26.4->-r requirements.txt (line 7)) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy==1.26.4->-r requirements.txt (line 7)) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy==1.26.4->-r requirements.txt (line 7)) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy==1.26.4->-r requirements.txt (line 7)) (2024.2.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed==0.12.3->-r requirements.txt (line 3)) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic->deepspeed==0.12.3->-r requirements.txt (line 3)) (2.29.0)\nRequirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from pynvml->deepspeed==0.12.3->-r requirements.txt (line 3)) (12.570.86)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.2->-r requirements.txt (line 13)) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.2->-r requirements.txt (line 13)) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.2->-r requirements.txt (line 13)) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.33.2->-r requirements.txt (line 13)) (2025.1.31)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy==1.26.4->-r requirements.txt (line 7)) (2024.2.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.34.2->-r requirements.txt (line 1)) (1.3.0)\nDownloading accelerate-0.34.2-py3-none-any.whl (324 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m324.4/324.4 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m66.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading matplotlib-3.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.3/8.3 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading peft-0.8.2-py3-none-any.whl (183 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.4/183.4 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.33.2-py3-none-any.whl (7.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m95.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m95.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading hjson-3.1.0-py3-none-any.whl (54 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: deepspeed\n  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for deepspeed: filename=deepspeed-0.12.3-py3-none-any.whl size=1279161 sha256=2ec9f0261a29dbcb60873eec06ff798528e30ceb32b17b76bd799363a072f20b\n  Stored in directory: /root/.cache/pip/wheels/ee/2b/c5/892ceee06964ce8aa2a98d4260848d0d9a3f1e743862e4b45a\nSuccessfully built deepspeed\nInstalling collected packages: tokenizers, sentencepiece, hjson, hydra-core, transformers, matplotlib, accelerate, tensorboard, peft, deepspeed, decord\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.21.0\n    Uninstalling tokenizers-0.21.0:\n      Successfully uninstalled tokenizers-0.21.0\n  Attempting uninstall: sentencepiece\n    Found existing installation: sentencepiece 0.2.0\n    Uninstalling sentencepiece-0.2.0:\n      Successfully uninstalled sentencepiece-0.2.0\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.0\n    Uninstalling transformers-4.47.0:\n      Successfully uninstalled transformers-4.47.0\n  Attempting uninstall: matplotlib\n    Found existing installation: matplotlib 3.7.5\n    Uninstalling matplotlib-3.7.5:\n      Successfully uninstalled matplotlib-3.7.5\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.2.1\n    Uninstalling accelerate-1.2.1:\n      Successfully uninstalled accelerate-1.2.1\n  Attempting uninstall: tensorboard\n    Found existing installation: tensorboard 2.17.1\n    Uninstalling tensorboard-2.17.1:\n      Successfully uninstalled tensorboard-2.17.1\n  Attempting uninstall: peft\n    Found existing installation: peft 0.14.0\n    Uninstalling peft-0.14.0:\n      Successfully uninstalled peft-0.14.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nmlxtend 0.23.3 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\nsentence-transformers 3.3.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.33.2 which is incompatible.\ntensorflow 2.17.1 requires tensorboard<2.18,>=2.17, but you have tensorboard 2.18.0 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed accelerate-0.34.2 decord-0.6.0 deepspeed-0.12.3 hjson-3.1.0 hydra-core-1.3.2 matplotlib-3.9.2 peft-0.8.2 sentencepiece-0.1.99 tensorboard-2.18.0 tokenizers-0.13.3 transformers-4.33.2\nNote: you may need to restart the kernel to use updated packages.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!python chat.py --version='MBZUAI/GeoPixel-7B-RES'"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-10T09:10:12.645457Z",
          "iopub.execute_input": "2025-03-10T09:10:12.645733Z",
          "iopub.status.idle": "2025-03-10T09:14:43.747237Z",
          "shell.execute_reply.started": "2025-03-10T09:10:12.645711Z",
          "shell.execute_reply": "2025-03-10T09:14:43.745972Z"
        },
        "id": "0NEObQmAjq8u",
        "outputId": "66409ce4-d1ea-4fce-80b6-e526e926332a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  torch.utils._pytree._register_pytree_node(\n2025-03-10 09:10:21.059445: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-03-10 09:10:21.270002: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2025-03-10 09:10:21.326671: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.10/dist-packages/transformers/utils/generic.py:311: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n  torch.utils._pytree._register_pytree_node(\ninitialing tokenizer from: MBZUAI/GeoPixel-7B-RES\n/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n  warnings.warn(\ntokenizer_config.json: 100%|███████████████| 2.38k/2.38k [00:00<00:00, 16.7MB/s]\ntokenization_internlm2.py: 100%|███████████| 8.81k/8.81k [00:00<00:00, 50.3MB/s]\nA new version of the following files was downloaded from https://huggingface.co/MBZUAI/GeoPixel-7B-RES:\n- tokenization_internlm2.py\n. Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\ntokenizer.model: 100%|█████████████████████| 1.48M/1.48M [00:00<00:00, 24.6MB/s]\nadded_tokens.json: 100%|███████████████████████| 209/209 [00:00<00:00, 1.74MB/s]\nspecial_tokens_map.json: 100%|█████████████████| 600/600 [00:00<00:00, 4.66MB/s]\nLoad model from: MBZUAI/GeoPixel-7B-RES\nconfig.json: 100%|█████████████████████████| 1.05k/1.05k [00:00<00:00, 6.82MB/s]\npytorch_model.bin.index.json: 100%|██████████| 277k/277k [00:00<00:00, 9.46MB/s]\nDownloading shards:   0%|                                 | 0/3 [00:00<?, ?it/s]\npytorch_model-00001-of-00003.bin:   0%|             | 0.00/9.97G [00:00<?, ?B/s]\u001b[A\npytorch_model-00001-of-00003.bin:   0%|     | 21.0M/9.97G [00:00<01:03, 157MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:   1%|     | 52.4M/9.97G [00:00<00:47, 207MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:   1%|     | 83.9M/9.97G [00:00<00:44, 221MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:   1%|      | 115M/9.97G [00:00<00:43, 228MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:   1%|      | 147M/9.97G [00:00<00:42, 233MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:   2%|      | 178M/9.97G [00:00<00:40, 242MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:   2%|▏     | 210M/9.97G [00:00<00:39, 246MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:   2%|▏     | 241M/9.97G [00:01<00:40, 240MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:   3%|▏     | 273M/9.97G [00:01<00:40, 241MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:   3%|▏     | 304M/9.97G [00:01<00:40, 239MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:   3%|▏     | 336M/9.97G [00:01<00:40, 239MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:   4%|▏     | 367M/9.97G [00:01<00:40, 239MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:   4%|▏     | 398M/9.97G [00:01<00:39, 240MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:   4%|▎     | 430M/9.97G [00:01<00:40, 236MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:   5%|▎     | 461M/9.97G [00:01<00:40, 237MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:   5%|▎     | 493M/9.97G [00:02<00:39, 241MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:   5%|▎     | 524M/9.97G [00:02<00:38, 243MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:   6%|▎     | 556M/9.97G [00:02<00:39, 241MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:   6%|▎     | 587M/9.97G [00:02<00:38, 243MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:   6%|▎     | 619M/9.97G [00:02<00:38, 245MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:   7%|▍     | 650M/9.97G [00:02<00:38, 243MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:   7%|▍     | 682M/9.97G [00:02<00:38, 243MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:   7%|▍     | 713M/9.97G [00:02<00:37, 245MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:   7%|▍     | 744M/9.97G [00:03<00:37, 245MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:   8%|▍     | 776M/9.97G [00:03<00:37, 247MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:   8%|▍     | 807M/9.97G [00:03<00:37, 243MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:   8%|▌     | 839M/9.97G [00:03<00:37, 241MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:   9%|▌     | 870M/9.97G [00:03<00:37, 242MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:   9%|▌     | 902M/9.97G [00:03<00:37, 243MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:   9%|▌     | 933M/9.97G [00:03<00:37, 244MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  10%|▌     | 965M/9.97G [00:04<00:37, 242MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  10%|▌     | 996M/9.97G [00:04<00:39, 225MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  10%|▌    | 1.03G/9.97G [00:04<00:41, 215MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  11%|▌    | 1.06G/9.97G [00:04<00:42, 208MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  11%|▌    | 1.09G/9.97G [00:04<00:40, 217MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  11%|▌    | 1.12G/9.97G [00:04<00:39, 225MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  12%|▌    | 1.15G/9.97G [00:04<00:38, 231MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  12%|▌    | 1.18G/9.97G [00:05<00:37, 234MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  12%|▌    | 1.22G/9.97G [00:05<00:36, 240MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  13%|▋    | 1.25G/9.97G [00:05<00:37, 232MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  13%|▋    | 1.28G/9.97G [00:05<00:47, 181MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  13%|▋    | 1.31G/9.97G [00:05<00:43, 197MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  13%|▋    | 1.34G/9.97G [00:05<00:41, 208MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  14%|▋    | 1.37G/9.97G [00:05<00:39, 216MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  14%|▋    | 1.41G/9.97G [00:06<00:38, 225MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  14%|▋    | 1.44G/9.97G [00:06<00:37, 229MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  15%|▋    | 1.47G/9.97G [00:06<00:47, 180MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  15%|▊    | 1.50G/9.97G [00:06<00:43, 197MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  15%|▊    | 1.53G/9.97G [00:06<00:40, 208MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  16%|▊    | 1.56G/9.97G [00:06<00:38, 217MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  16%|▊    | 1.59G/9.97G [00:06<00:37, 221MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  16%|▊    | 1.63G/9.97G [00:07<00:36, 226MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  17%|▊    | 1.66G/9.97G [00:07<00:35, 231MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  17%|▊    | 1.69G/9.97G [00:07<00:45, 181MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  17%|▊    | 1.72G/9.97G [00:07<00:41, 197MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  18%|▉    | 1.75G/9.97G [00:07<00:39, 209MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  18%|▉    | 1.78G/9.97G [00:07<00:37, 217MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  18%|▉    | 1.81G/9.97G [00:08<00:36, 224MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  19%|▉    | 1.85G/9.97G [00:08<00:35, 227MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  19%|▉    | 1.88G/9.97G [00:08<00:35, 231MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  19%|▉    | 1.91G/9.97G [00:08<00:44, 183MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  19%|▉    | 1.94G/9.97G [00:08<00:40, 197MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  20%|▉    | 1.97G/9.97G [00:08<00:38, 207MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  20%|█    | 2.00G/9.97G [00:08<00:36, 216MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  20%|█    | 2.03G/9.97G [00:09<00:35, 224MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  21%|█    | 2.07G/9.97G [00:09<00:34, 227MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  21%|█    | 2.10G/9.97G [00:09<00:43, 180MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  21%|█    | 2.13G/9.97G [00:09<00:40, 192MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  22%|█    | 2.16G/9.97G [00:09<00:38, 202MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  22%|█    | 2.19G/9.97G [00:09<00:36, 211MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  22%|█    | 2.22G/9.97G [00:10<00:35, 219MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  23%|█▏   | 2.25G/9.97G [00:10<00:36, 210MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  23%|█▏   | 2.29G/9.97G [00:10<00:46, 164MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  23%|█▏   | 2.31G/9.97G [00:10<00:51, 149MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  23%|█▏   | 2.34G/9.97G [00:10<00:49, 153MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  24%|█▏   | 2.36G/9.97G [00:11<00:51, 146MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  24%|█▏   | 2.38G/9.97G [00:11<00:55, 136MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  24%|█▏   | 2.41G/9.97G [00:11<00:48, 155MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  24%|█▏   | 2.43G/9.97G [00:11<00:50, 150MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  25%|█▏   | 2.45G/9.97G [00:11<00:48, 156MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  25%|█▏   | 2.47G/9.97G [00:11<00:44, 167MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  25%|█▎   | 2.51G/9.97G [00:11<00:40, 183MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  25%|█▎   | 2.53G/9.97G [00:12<00:46, 161MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  26%|█▎   | 2.55G/9.97G [00:12<00:52, 142MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  26%|█▎   | 2.57G/9.97G [00:12<00:48, 151MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  26%|█▎   | 2.59G/9.97G [00:12<00:45, 163MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  26%|█▎   | 2.62G/9.97G [00:12<00:41, 176MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  27%|█▎   | 2.65G/9.97G [00:12<00:42, 172MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  27%|█▎   | 2.67G/9.97G [00:12<00:46, 158MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  27%|█▎   | 2.69G/9.97G [00:13<00:53, 135MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  27%|█▎   | 2.73G/9.97G [00:13<00:44, 161MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  28%|█▍   | 2.76G/9.97G [00:13<00:39, 182MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  28%|█▍   | 2.79G/9.97G [00:13<00:36, 198MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  28%|█▍   | 2.82G/9.97G [00:13<00:34, 210MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  29%|█▍   | 2.85G/9.97G [00:13<00:32, 221MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  29%|█▍   | 2.88G/9.97G [00:14<00:36, 195MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  29%|█▍   | 2.90G/9.97G [00:14<00:35, 197MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  29%|█▍   | 2.93G/9.97G [00:14<00:35, 199MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  30%|█▍   | 2.95G/9.97G [00:14<00:43, 161MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  30%|█▍   | 2.98G/9.97G [00:14<00:39, 177MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  30%|█▌   | 3.00G/9.97G [00:14<00:41, 166MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  30%|█▌   | 3.02G/9.97G [00:14<00:43, 162MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  31%|█▌   | 3.04G/9.97G [00:15<00:47, 146MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  31%|█▌   | 3.06G/9.97G [00:15<00:47, 145MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  31%|█▌   | 3.08G/9.97G [00:15<00:52, 131MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  31%|█▌   | 3.10G/9.97G [00:15<00:53, 129MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  31%|█▌   | 3.14G/9.97G [00:15<00:43, 157MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  32%|█▌   | 3.16G/9.97G [00:15<00:41, 164MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  32%|█▌   | 3.18G/9.97G [00:15<00:46, 146MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  32%|█▌   | 3.20G/9.97G [00:16<00:49, 136MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  32%|█▌   | 3.23G/9.97G [00:16<00:41, 162MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  33%|█▋   | 3.26G/9.97G [00:16<00:36, 183MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  33%|█▋   | 3.29G/9.97G [00:16<00:33, 200MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  33%|█▋   | 3.32G/9.97G [00:16<00:32, 207MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  34%|█▋   | 3.36G/9.97G [00:16<00:30, 219MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  34%|█▋   | 3.39G/9.97G [00:16<00:29, 224MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  34%|█▋   | 3.42G/9.97G [00:17<00:37, 176MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  35%|█▋   | 3.44G/9.97G [00:17<00:37, 175MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  35%|█▋   | 3.46G/9.97G [00:17<00:38, 170MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  35%|█▋   | 3.48G/9.97G [00:17<00:36, 179MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  35%|█▊   | 3.50G/9.97G [00:17<00:35, 181MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  35%|█▊   | 3.52G/9.97G [00:17<00:38, 167MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  36%|█▊   | 3.54G/9.97G [00:18<00:41, 153MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  36%|█▊   | 3.57G/9.97G [00:18<00:41, 155MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  36%|█▊   | 3.59G/9.97G [00:18<00:39, 162MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  36%|█▊   | 3.61G/9.97G [00:18<00:36, 173MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  37%|█▊   | 3.64G/9.97G [00:18<00:33, 191MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  37%|█▊   | 3.66G/9.97G [00:18<00:35, 179MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  37%|█▊   | 3.69G/9.97G [00:18<00:33, 188MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  37%|█▊   | 3.72G/9.97G [00:18<00:30, 202MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  38%|█▉   | 3.75G/9.97G [00:19<00:29, 211MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  38%|█▉   | 3.79G/9.97G [00:19<00:29, 212MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  38%|█▉   | 3.82G/9.97G [00:19<00:28, 219MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  39%|█▉   | 3.85G/9.97G [00:19<00:27, 221MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  39%|█▉   | 3.88G/9.97G [00:19<00:27, 223MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  39%|█▉   | 3.91G/9.97G [00:19<00:26, 226MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  40%|█▉   | 3.94G/9.97G [00:19<00:26, 229MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  40%|█▉   | 3.97G/9.97G [00:20<00:26, 229MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  40%|██   | 4.01G/9.97G [00:20<00:26, 228MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  40%|██   | 4.04G/9.97G [00:20<00:25, 231MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  41%|██   | 4.07G/9.97G [00:20<00:25, 231MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  41%|██   | 4.10G/9.97G [00:20<00:25, 229MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  41%|██   | 4.13G/9.97G [00:20<00:25, 231MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  42%|██   | 4.16G/9.97G [00:20<00:25, 228MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  42%|██   | 4.19G/9.97G [00:20<00:24, 236MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  42%|██   | 4.23G/9.97G [00:21<00:23, 240MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  43%|██▏  | 4.26G/9.97G [00:21<00:23, 247MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  43%|██▏  | 4.29G/9.97G [00:21<00:23, 245MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  43%|██▏  | 4.32G/9.97G [00:21<00:23, 244MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  44%|██▏  | 4.35G/9.97G [00:21<00:23, 244MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  44%|██▏  | 4.38G/9.97G [00:21<00:23, 242MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  44%|██▏  | 4.41G/9.97G [00:21<00:29, 186MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  44%|██▏  | 4.44G/9.97G [00:22<00:29, 188MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  45%|██▏  | 4.46G/9.97G [00:22<00:41, 132MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  45%|██▎  | 4.49G/9.97G [00:22<00:35, 154MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  45%|██▎  | 4.52G/9.97G [00:22<00:30, 179MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  46%|██▎  | 4.55G/9.97G [00:22<00:27, 200MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  46%|██▎  | 4.58G/9.97G [00:22<00:25, 213MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  46%|██▎  | 4.61G/9.97G [00:23<00:23, 224MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  47%|██▎  | 4.65G/9.97G [00:23<00:23, 229MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  47%|██▎  | 4.68G/9.97G [00:23<00:22, 234MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  47%|██▎  | 4.71G/9.97G [00:23<00:25, 203MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  48%|██▍  | 4.74G/9.97G [00:23<00:25, 209MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  48%|██▍  | 4.77G/9.97G [00:23<00:23, 218MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  48%|██▍  | 4.80G/9.97G [00:23<00:23, 223MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  48%|██▍  | 4.83G/9.97G [00:24<00:22, 230MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  49%|██▍  | 4.87G/9.97G [00:24<00:21, 233MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  49%|██▍  | 4.90G/9.97G [00:24<00:21, 239MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  49%|██▍  | 4.93G/9.97G [00:24<00:20, 245MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  50%|██▍  | 4.96G/9.97G [00:24<00:20, 244MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  50%|██▌  | 4.99G/9.97G [00:24<00:20, 244MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  50%|██▌  | 5.02G/9.97G [00:24<00:19, 250MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  51%|██▌  | 5.05G/9.97G [00:24<00:19, 246MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  51%|██▌  | 5.09G/9.97G [00:25<00:19, 246MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  51%|██▌  | 5.12G/9.97G [00:25<00:19, 245MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  52%|██▌  | 5.15G/9.97G [00:25<00:19, 242MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  52%|██▌  | 5.18G/9.97G [00:25<00:20, 238MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  52%|██▌  | 5.21G/9.97G [00:25<00:19, 239MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  53%|██▋  | 5.24G/9.97G [00:25<00:19, 240MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  53%|██▋  | 5.27G/9.97G [00:25<00:19, 242MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  53%|██▋  | 5.31G/9.97G [00:25<00:19, 243MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  54%|██▋  | 5.34G/9.97G [00:26<00:18, 245MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  54%|██▋  | 5.37G/9.97G [00:26<00:18, 248MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  54%|██▋  | 5.40G/9.97G [00:26<00:18, 249MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  54%|██▋  | 5.43G/9.97G [00:26<00:18, 243MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  55%|██▋  | 5.46G/9.97G [00:26<00:18, 243MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  55%|██▊  | 5.49G/9.97G [00:26<00:18, 242MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  55%|██▊  | 5.53G/9.97G [00:26<00:18, 240MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  56%|██▊  | 5.56G/9.97G [00:26<00:18, 238MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  56%|██▊  | 5.59G/9.97G [00:27<00:18, 239MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  56%|██▊  | 5.62G/9.97G [00:27<00:17, 243MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  57%|██▊  | 5.65G/9.97G [00:27<00:17, 240MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  57%|██▊  | 5.68G/9.97G [00:27<00:17, 239MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  57%|██▊  | 5.71G/9.97G [00:27<00:18, 234MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  58%|██▉  | 5.75G/9.97G [00:27<00:17, 241MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  58%|██▉  | 5.78G/9.97G [00:27<00:17, 242MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  58%|██▉  | 5.81G/9.97G [00:28<00:17, 231MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  59%|██▉  | 5.84G/9.97G [00:28<00:17, 236MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  59%|██▉  | 5.87G/9.97G [00:28<00:17, 235MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  59%|██▉  | 5.90G/9.97G [00:28<00:22, 179MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  59%|██▉  | 5.92G/9.97G [00:28<00:23, 171MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  60%|██▉  | 5.95G/9.97G [00:28<00:23, 168MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  60%|██▉  | 5.97G/9.97G [00:29<00:24, 161MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  60%|███  | 5.99G/9.97G [00:29<00:24, 161MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  60%|███  | 6.01G/9.97G [00:29<00:27, 143MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  61%|███  | 6.04G/9.97G [00:29<00:25, 157MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  61%|███  | 6.07G/9.97G [00:29<00:22, 174MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  61%|███  | 6.10G/9.97G [00:29<00:22, 175MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  61%|███  | 6.12G/9.97G [00:29<00:23, 161MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  62%|███  | 6.14G/9.97G [00:30<00:23, 161MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  62%|███  | 6.17G/9.97G [00:30<00:23, 165MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  62%|███  | 6.19G/9.97G [00:30<00:22, 166MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  62%|███  | 6.21G/9.97G [00:30<00:23, 157MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  62%|███  | 6.23G/9.97G [00:30<00:23, 157MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  63%|███▏ | 6.25G/9.97G [00:30<00:28, 133MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  63%|███▏ | 6.27G/9.97G [00:31<00:29, 125MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  63%|███▏ | 6.29G/9.97G [00:31<00:27, 135MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  63%|███▏ | 6.32G/9.97G [00:31<00:22, 160MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  64%|███▏ | 6.34G/9.97G [00:31<00:22, 159MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  64%|███▏ | 6.36G/9.97G [00:31<00:27, 131MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  64%|███▏ | 6.40G/9.97G [00:31<00:24, 145MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  64%|███▏ | 6.43G/9.97G [00:32<00:22, 160MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  65%|███▏ | 6.45G/9.97G [00:32<00:22, 155MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  65%|███▎ | 6.48G/9.97G [00:32<00:21, 165MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  65%|███▎ | 6.50G/9.97G [00:32<00:22, 152MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  65%|███▎ | 6.52G/9.97G [00:32<00:24, 140MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  66%|███▎ | 6.55G/9.97G [00:32<00:22, 153MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  66%|███▎ | 6.57G/9.97G [00:33<00:22, 149MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  66%|███▎ | 6.61G/9.97G [00:33<00:19, 173MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  67%|███▎ | 6.64G/9.97G [00:33<00:17, 190MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  67%|███▎ | 6.66G/9.97G [00:33<00:26, 125MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  67%|███▎ | 6.68G/9.97G [00:33<00:23, 138MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  67%|███▎ | 6.71G/9.97G [00:33<00:20, 161MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  68%|███▍ | 6.73G/9.97G [00:33<00:19, 168MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  68%|███▍ | 6.75G/9.97G [00:34<00:19, 164MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  68%|███▍ | 6.77G/9.97G [00:34<00:20, 156MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  68%|███▍ | 6.81G/9.97G [00:34<00:18, 174MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  69%|███▍ | 6.84G/9.97G [00:34<00:16, 191MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  69%|███▍ | 6.86G/9.97G [00:34<00:16, 194MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  69%|███▍ | 6.88G/9.97G [00:34<00:17, 177MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  69%|███▍ | 6.91G/9.97G [00:34<00:16, 182MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  70%|███▍ | 6.93G/9.97G [00:35<00:20, 145MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  70%|███▍ | 6.95G/9.97G [00:35<00:20, 149MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  70%|███▍ | 6.97G/9.97G [00:35<00:19, 153MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  70%|███▌ | 6.99G/9.97G [00:35<00:20, 143MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  70%|███▌ | 7.01G/9.97G [00:35<00:19, 149MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  71%|███▌ | 7.05G/9.97G [00:35<00:16, 174MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  71%|███▌ | 7.08G/9.97G [00:36<00:14, 193MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  71%|███▌ | 7.11G/9.97G [00:36<00:13, 207MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  72%|███▌ | 7.14G/9.97G [00:36<00:12, 218MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  72%|███▌ | 7.17G/9.97G [00:36<00:12, 226MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  72%|███▌ | 7.20G/9.97G [00:36<00:12, 230MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  73%|███▋ | 7.24G/9.97G [00:36<00:11, 233MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  73%|███▋ | 7.27G/9.97G [00:36<00:11, 237MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  73%|███▋ | 7.30G/9.97G [00:36<00:11, 238MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  74%|███▋ | 7.33G/9.97G [00:37<00:11, 237MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  74%|███▋ | 7.36G/9.97G [00:37<00:10, 239MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  74%|███▋ | 7.39G/9.97G [00:37<00:10, 239MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  74%|███▋ | 7.42G/9.97G [00:37<00:10, 242MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  75%|███▋ | 7.46G/9.97G [00:37<00:10, 245MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  75%|███▊ | 7.49G/9.97G [00:37<00:10, 243MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  75%|███▊ | 7.52G/9.97G [00:37<00:10, 241MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  76%|███▊ | 7.55G/9.97G [00:37<00:10, 238MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  76%|███▊ | 7.58G/9.97G [00:38<00:09, 240MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  76%|███▊ | 7.61G/9.97G [00:38<00:09, 239MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  77%|███▊ | 7.64G/9.97G [00:38<00:09, 240MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  77%|███▊ | 7.68G/9.97G [00:38<00:09, 242MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  77%|███▊ | 7.71G/9.97G [00:38<00:09, 240MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  78%|███▉ | 7.74G/9.97G [00:38<00:09, 240MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  78%|███▉ | 7.77G/9.97G [00:38<00:09, 240MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  78%|███▉ | 7.80G/9.97G [00:39<00:08, 242MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  79%|███▉ | 7.83G/9.97G [00:39<00:08, 242MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  79%|███▉ | 7.86G/9.97G [00:39<00:08, 240MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  79%|███▉ | 7.90G/9.97G [00:39<00:08, 241MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  80%|███▏| 7.93G/9.97G [00:41<00:39, 51.5MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  80%|███▏| 7.96G/9.97G [00:41<00:30, 66.8MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  80%|███▏| 7.99G/9.97G [00:41<00:23, 84.7MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  80%|████ | 8.02G/9.97G [00:41<00:18, 105MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  81%|████ | 8.05G/9.97G [00:41<00:15, 126MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  81%|████ | 8.08G/9.97G [00:41<00:13, 145MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  81%|████ | 8.12G/9.97G [00:41<00:11, 157MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  82%|████ | 8.15G/9.97G [00:42<00:10, 175MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  82%|████ | 8.18G/9.97G [00:42<00:09, 191MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  82%|████ | 8.21G/9.97G [00:42<00:08, 209MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  83%|████▏| 8.24G/9.97G [00:42<00:07, 219MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  83%|████▏| 8.27G/9.97G [00:42<00:07, 223MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  83%|████▏| 8.30G/9.97G [00:42<00:07, 229MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  84%|████▏| 8.34G/9.97G [00:42<00:07, 231MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  84%|████▏| 8.37G/9.97G [00:43<00:07, 226MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  84%|████▏| 8.40G/9.97G [00:43<00:06, 227MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  85%|████▏| 8.43G/9.97G [00:43<00:06, 228MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  85%|████▏| 8.46G/9.97G [00:43<00:06, 230MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  85%|████▎| 8.49G/9.97G [00:43<00:06, 230MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  86%|████▎| 8.52G/9.97G [00:43<00:09, 159MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  86%|████▎| 8.56G/9.97G [00:44<00:07, 181MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  86%|████▎| 8.59G/9.97G [00:44<00:06, 201MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  86%|████▎| 8.62G/9.97G [00:44<00:06, 217MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  87%|████▎| 8.65G/9.97G [00:44<00:05, 229MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  87%|████▎| 8.68G/9.97G [00:44<00:05, 238MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  87%|████▎| 8.71G/9.97G [00:44<00:05, 236MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  88%|████▍| 8.75G/9.97G [00:44<00:05, 240MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  88%|████▍| 8.78G/9.97G [00:44<00:04, 245MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  88%|████▍| 8.81G/9.97G [00:45<00:04, 238MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  89%|████▍| 8.84G/9.97G [00:45<00:04, 236MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  89%|████▍| 8.87G/9.97G [00:45<00:04, 233MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  89%|████▍| 8.90G/9.97G [00:45<00:04, 236MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  90%|████▍| 8.93G/9.97G [00:45<00:04, 236MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  90%|████▍| 8.97G/9.97G [00:45<00:04, 240MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  90%|████▌| 9.00G/9.97G [00:45<00:04, 243MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  91%|████▌| 9.03G/9.97G [00:45<00:04, 226MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  91%|████▌| 9.06G/9.97G [00:46<00:04, 187MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  91%|████▌| 9.08G/9.97G [00:46<00:05, 177MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  91%|████▌| 9.11G/9.97G [00:46<00:04, 195MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  92%|████▌| 9.13G/9.97G [00:46<00:05, 156MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  92%|████▌| 9.15G/9.97G [00:46<00:05, 151MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  92%|████▌| 9.18G/9.97G [00:47<00:06, 128MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  92%|████▌| 9.20G/9.97G [00:47<00:06, 115MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  93%|████▋| 9.23G/9.97G [00:47<00:05, 143MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  93%|████▋| 9.26G/9.97G [00:47<00:04, 163MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  93%|████▋| 9.29G/9.97G [00:47<00:03, 179MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  94%|████▋| 9.32G/9.97G [00:47<00:03, 194MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  94%|████▋| 9.35G/9.97G [00:48<00:02, 206MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  94%|████▋| 9.38G/9.97G [00:48<00:02, 217MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  94%|████▋| 9.42G/9.97G [00:48<00:02, 224MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  95%|████▋| 9.45G/9.97G [00:48<00:02, 228MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  95%|████▊| 9.48G/9.97G [00:48<00:02, 232MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  95%|████▊| 9.51G/9.97G [00:48<00:01, 235MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  96%|████▊| 9.54G/9.97G [00:48<00:01, 236MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  96%|████▊| 9.57G/9.97G [00:48<00:01, 217MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  96%|████▊| 9.60G/9.97G [00:49<00:01, 225MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  97%|████▊| 9.64G/9.97G [00:49<00:01, 177MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  97%|████▊| 9.66G/9.97G [00:49<00:02, 155MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  97%|████▊| 9.68G/9.97G [00:49<00:01, 145MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  97%|████▊| 9.70G/9.97G [00:49<00:01, 149MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  98%|████▉| 9.72G/9.97G [00:50<00:01, 142MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  98%|████▉| 9.75G/9.97G [00:50<00:01, 168MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  98%|████▉| 9.77G/9.97G [00:50<00:01, 142MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  98%|████▉| 9.80G/9.97G [00:50<00:00, 165MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  99%|████▉| 9.84G/9.97G [00:50<00:00, 187MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  99%|████▉| 9.87G/9.97G [00:50<00:00, 202MB/s]\u001b[A\npytorch_model-00001-of-00003.bin:  99%|████▉| 9.90G/9.97G [00:50<00:00, 183MB/s]\u001b[A\npytorch_model-00001-of-00003.bin: 100%|████▉| 9.92G/9.97G [00:51<00:00, 168MB/s]\u001b[A\npytorch_model-00001-of-00003.bin: 100%|████▉| 9.94G/9.97G [00:51<00:00, 166MB/s]\u001b[A\npytorch_model-00001-of-00003.bin: 100%|█████| 9.97G/9.97G [00:51<00:00, 194MB/s]\u001b[A\nDownloading shards:  33%|████████▎                | 1/3 [00:51<01:43, 51.55s/it]\npytorch_model-00002-of-00003.bin:   0%|             | 0.00/10.0G [00:00<?, ?B/s]\u001b[A\npytorch_model-00002-of-00003.bin:   0%|     | 21.0M/10.0G [00:00<00:59, 167MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   0%|     | 41.9M/10.0G [00:00<00:52, 188MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   1%|     | 62.9M/10.0G [00:00<00:50, 196MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   1%|     | 94.4M/10.0G [00:00<00:47, 210MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   1%|      | 126M/10.0G [00:00<00:45, 219MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   2%|      | 157M/10.0G [00:00<00:43, 228MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   2%|      | 189M/10.0G [00:00<00:43, 226MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   2%|▏     | 220M/10.0G [00:01<00:42, 231MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   3%|▏     | 252M/10.0G [00:01<00:41, 233MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   3%|▏     | 283M/10.0G [00:01<00:40, 238MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   3%|▏     | 315M/10.0G [00:01<00:40, 237MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   3%|▏     | 346M/10.0G [00:01<00:40, 237MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   4%|▏     | 377M/10.0G [00:01<00:40, 238MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   4%|▏     | 409M/10.0G [00:01<00:39, 241MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   4%|▎     | 440M/10.0G [00:01<00:40, 236MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   5%|▎     | 472M/10.0G [00:02<00:40, 234MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   5%|▎     | 503M/10.0G [00:02<00:40, 234MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   5%|▎     | 535M/10.0G [00:02<00:40, 234MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   6%|▎     | 566M/10.0G [00:02<00:40, 233MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   6%|▎     | 598M/10.0G [00:02<00:39, 236MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   6%|▍     | 629M/10.0G [00:02<00:49, 189MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   7%|▍     | 661M/10.0G [00:02<00:47, 198MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   7%|▍     | 692M/10.0G [00:03<00:44, 208MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   7%|▍     | 724M/10.0G [00:03<00:42, 219MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   8%|▍     | 755M/10.0G [00:03<00:40, 229MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   8%|▍     | 786M/10.0G [00:03<00:39, 233MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   8%|▍     | 818M/10.0G [00:03<00:38, 236MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   8%|▌     | 849M/10.0G [00:03<00:49, 184MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   9%|▌     | 881M/10.0G [00:04<00:46, 198MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   9%|▌     | 912M/10.0G [00:04<00:44, 205MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:   9%|▌     | 944M/10.0G [00:04<00:42, 213MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  10%|▌     | 975M/10.0G [00:04<00:40, 223MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  10%|▌    | 1.01G/10.0G [00:04<00:39, 228MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  10%|▌    | 1.04G/10.0G [00:04<00:38, 233MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  11%|▌    | 1.07G/10.0G [00:04<00:48, 185MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  11%|▌    | 1.10G/10.0G [00:05<00:44, 199MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  11%|▌    | 1.13G/10.0G [00:05<00:41, 212MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  12%|▌    | 1.16G/10.0G [00:05<00:39, 223MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  12%|▌    | 1.20G/10.0G [00:05<00:38, 227MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  12%|▌    | 1.23G/10.0G [00:05<00:38, 230MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  13%|▋    | 1.26G/10.0G [00:05<00:49, 177MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  13%|▋    | 1.29G/10.0G [00:05<00:45, 193MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  13%|▋    | 1.32G/10.0G [00:06<00:42, 205MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  14%|▋    | 1.35G/10.0G [00:06<00:40, 214MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  14%|▋    | 1.38G/10.0G [00:06<00:39, 221MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  14%|▋    | 1.42G/10.0G [00:06<00:37, 227MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  14%|▋    | 1.45G/10.0G [00:06<00:36, 231MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  15%|▋    | 1.48G/10.0G [00:06<00:46, 182MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  15%|▊    | 1.51G/10.0G [00:07<00:48, 177MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  15%|▊    | 1.53G/10.0G [00:07<00:54, 156MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  16%|▊    | 1.55G/10.0G [00:07<00:57, 148MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  16%|▊    | 1.57G/10.0G [00:07<00:54, 155MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  16%|▊    | 1.59G/10.0G [00:07<00:52, 160MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  16%|▊    | 1.63G/10.0G [00:07<00:46, 181MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  17%|▊    | 1.66G/10.0G [00:07<00:42, 195MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  17%|▊    | 1.69G/10.0G [00:08<00:40, 204MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  17%|▊    | 1.72G/10.0G [00:08<00:38, 214MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  18%|▉    | 1.75G/10.0G [00:08<00:38, 216MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  18%|▉    | 1.78G/10.0G [00:08<00:49, 166MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  18%|▉    | 1.80G/10.0G [00:08<00:47, 171MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  18%|▉    | 1.84G/10.0G [00:08<00:43, 190MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  19%|▉    | 1.87G/10.0G [00:09<00:39, 204MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  19%|▉    | 1.90G/10.0G [00:09<00:37, 216MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  19%|▉    | 1.93G/10.0G [00:09<00:38, 209MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  20%|▉    | 1.96G/10.0G [00:09<00:41, 194MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  20%|▉    | 1.98G/10.0G [00:09<00:51, 155MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  20%|█    | 2.00G/10.0G [00:09<00:52, 154MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  20%|█    | 2.02G/10.0G [00:10<00:53, 150MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  20%|█    | 2.04G/10.0G [00:10<00:55, 144MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  21%|█    | 2.08G/10.0G [00:10<00:47, 168MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  21%|█    | 2.11G/10.0G [00:10<00:41, 190MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  21%|█    | 2.14G/10.0G [00:10<00:38, 202MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  22%|█    | 2.17G/10.0G [00:10<00:37, 206MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  22%|█    | 2.20G/10.0G [00:10<00:36, 213MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  22%|█    | 2.23G/10.0G [00:10<00:35, 221MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  23%|█▏   | 2.26G/10.0G [00:11<00:33, 228MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  23%|█▏   | 2.30G/10.0G [00:11<00:33, 230MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  23%|█▏   | 2.33G/10.0G [00:11<00:33, 229MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  24%|█▏   | 2.36G/10.0G [00:11<00:32, 232MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  24%|█▏   | 2.39G/10.0G [00:11<00:32, 237MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  24%|█▏   | 2.42G/10.0G [00:11<00:31, 239MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  25%|█▏   | 2.45G/10.0G [00:12<00:46, 162MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  25%|█▏   | 2.49G/10.0G [00:12<00:41, 179MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  25%|█▎   | 2.52G/10.0G [00:12<00:38, 196MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  25%|█▎   | 2.55G/10.0G [00:12<00:35, 208MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  26%|█▎   | 2.58G/10.0G [00:12<00:34, 218MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  26%|█▎   | 2.61G/10.0G [00:12<00:33, 224MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  26%|█▎   | 2.64G/10.0G [00:12<00:32, 230MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  27%|█▎   | 2.67G/10.0G [00:12<00:31, 236MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  27%|█▎   | 2.71G/10.0G [00:13<00:29, 244MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  27%|█▎   | 2.74G/10.0G [00:13<00:30, 241MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  28%|█▍   | 2.77G/10.0G [00:13<00:29, 245MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  28%|█▍   | 2.80G/10.0G [00:13<00:29, 243MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  28%|█▍   | 2.83G/10.0G [00:13<00:29, 243MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  29%|█▍   | 2.86G/10.0G [00:13<00:29, 245MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  29%|█▍   | 2.89G/10.0G [00:13<00:29, 241MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  29%|█▍   | 2.93G/10.0G [00:14<00:29, 238MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  30%|█▍   | 2.96G/10.0G [00:14<00:29, 240MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  30%|█▍   | 2.99G/10.0G [00:14<00:29, 241MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  30%|█▌   | 3.02G/10.0G [00:14<00:29, 240MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  31%|█▌   | 3.05G/10.0G [00:14<00:28, 243MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  31%|█▌   | 3.08G/10.0G [00:14<00:28, 241MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  31%|█▌   | 3.11G/10.0G [00:14<00:28, 239MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  31%|█▌   | 3.15G/10.0G [00:14<00:28, 240MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  32%|█▌   | 3.18G/10.0G [00:15<00:28, 243MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  32%|█▌   | 3.21G/10.0G [00:15<00:27, 244MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  32%|█▌   | 3.24G/10.0G [00:15<00:27, 243MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  33%|█▋   | 3.27G/10.0G [00:15<00:27, 244MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  33%|█▋   | 3.30G/10.0G [00:15<00:27, 245MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  33%|█▋   | 3.33G/10.0G [00:15<00:27, 244MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  34%|█▋   | 3.37G/10.0G [00:15<00:29, 223MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  34%|█▋   | 3.40G/10.0G [00:16<00:29, 222MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  34%|█▋   | 3.43G/10.0G [00:16<00:29, 225MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  35%|█▋   | 3.46G/10.0G [00:16<00:28, 228MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  35%|█▋   | 3.49G/10.0G [00:16<00:28, 230MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  35%|█▊   | 3.52G/10.0G [00:16<00:27, 235MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  36%|█▊   | 3.55G/10.0G [00:16<00:27, 237MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  36%|█▊   | 3.59G/10.0G [00:16<00:33, 191MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  36%|█▊   | 3.62G/10.0G [00:17<00:31, 205MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  36%|█▊   | 3.65G/10.0G [00:17<00:29, 214MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  37%|█▊   | 3.68G/10.0G [00:17<00:28, 219MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  37%|█▊   | 3.71G/10.0G [00:17<00:29, 214MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  37%|█▍  | 3.74G/10.0G [00:20<03:10, 32.8MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  38%|█▌  | 3.77G/10.0G [00:20<02:21, 44.1MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  38%|█▌  | 3.81G/10.0G [00:20<01:45, 58.8MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  38%|█▌  | 3.84G/10.0G [00:20<01:20, 76.8MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  39%|█▌  | 3.87G/10.0G [00:20<01:02, 97.4MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  39%|█▉   | 3.90G/10.0G [00:21<00:57, 106MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  39%|█▉   | 3.92G/10.0G [00:21<00:51, 118MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  40%|█▉   | 3.95G/10.0G [00:21<00:45, 132MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  40%|█▉   | 3.97G/10.0G [00:21<00:45, 133MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  40%|█▉   | 4.00G/10.0G [00:21<00:44, 135MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  40%|██   | 4.03G/10.0G [00:21<00:37, 157MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  41%|██   | 4.06G/10.0G [00:21<00:33, 176MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  41%|██   | 4.09G/10.0G [00:22<00:31, 188MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  41%|██   | 4.12G/10.0G [00:22<00:29, 201MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  42%|██   | 4.15G/10.0G [00:22<00:28, 208MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  42%|██   | 4.18G/10.0G [00:22<00:32, 180MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  42%|██   | 4.20G/10.0G [00:22<00:32, 176MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  42%|██   | 4.24G/10.0G [00:22<00:30, 192MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  43%|██▏  | 4.27G/10.0G [00:23<00:31, 184MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  43%|██▏  | 4.29G/10.0G [00:23<00:46, 123MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  43%|██▏  | 4.31G/10.0G [00:23<00:49, 114MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  43%|██▏  | 4.33G/10.0G [00:23<00:44, 127MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  44%|██▏  | 4.35G/10.0G [00:23<00:43, 131MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  44%|██▏  | 4.37G/10.0G [00:24<00:43, 129MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  44%|██▏  | 4.40G/10.0G [00:24<00:36, 154MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  44%|██▏  | 4.42G/10.0G [00:24<00:34, 161MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  45%|██▏  | 4.46G/10.0G [00:24<00:30, 181MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  45%|██▏  | 4.48G/10.0G [00:24<00:29, 187MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  45%|██▎  | 4.51G/10.0G [00:24<00:27, 201MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  45%|██▎  | 4.54G/10.0G [00:24<00:26, 205MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  46%|██▎  | 4.56G/10.0G [00:24<00:28, 191MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  46%|██▎  | 4.58G/10.0G [00:25<00:36, 150MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  46%|██▎  | 4.60G/10.0G [00:25<00:34, 158MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  46%|██▎  | 4.63G/10.0G [00:25<00:30, 177MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  47%|██▎  | 4.67G/10.0G [00:25<00:27, 194MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  47%|██▎  | 4.70G/10.0G [00:25<00:25, 209MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  47%|██▎  | 4.73G/10.0G [00:25<00:24, 217MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  48%|██▍  | 4.76G/10.0G [00:25<00:23, 226MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  48%|██▍  | 4.79G/10.0G [00:26<00:22, 233MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  48%|██▍  | 4.82G/10.0G [00:26<00:21, 238MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  49%|██▍  | 4.85G/10.0G [00:26<00:20, 246MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  49%|██▍  | 4.89G/10.0G [00:26<00:20, 249MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  49%|██▍  | 4.92G/10.0G [00:26<00:20, 249MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  49%|██▍  | 4.95G/10.0G [00:26<00:21, 239MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  50%|██▍  | 4.98G/10.0G [00:26<00:20, 245MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  50%|██▌  | 5.01G/10.0G [00:26<00:20, 243MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  50%|██▌  | 5.04G/10.0G [00:27<00:20, 243MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  51%|██▌  | 5.08G/10.0G [00:27<00:20, 245MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  51%|██▌  | 5.11G/10.0G [00:27<00:20, 239MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  51%|██▌  | 5.14G/10.0G [00:27<00:20, 238MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  52%|██▌  | 5.17G/10.0G [00:27<00:20, 237MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  52%|██▌  | 5.20G/10.0G [00:27<00:20, 235MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  52%|██▌  | 5.23G/10.0G [00:27<00:20, 237MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  53%|██▋  | 5.26G/10.0G [00:28<00:20, 233MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  53%|██▋  | 5.30G/10.0G [00:28<00:20, 231MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  53%|██▋  | 5.33G/10.0G [00:28<00:20, 232MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  54%|██▋  | 5.36G/10.0G [00:28<00:19, 233MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  54%|██▋  | 5.39G/10.0G [00:28<00:19, 235MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  54%|██▋  | 5.42G/10.0G [00:28<00:19, 239MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  55%|██▋  | 5.45G/10.0G [00:28<00:18, 242MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  55%|██▋  | 5.48G/10.0G [00:28<00:18, 240MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  55%|██▊  | 5.52G/10.0G [00:29<00:18, 242MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  55%|██▊  | 5.55G/10.0G [00:29<00:18, 237MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  56%|██▊  | 5.58G/10.0G [00:29<00:19, 231MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  56%|██▊  | 5.61G/10.0G [00:29<00:18, 232MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  56%|██▊  | 5.64G/10.0G [00:29<00:18, 230MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  57%|██▊  | 5.67G/10.0G [00:29<00:18, 230MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  57%|██▊  | 5.70G/10.0G [00:29<00:18, 228MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  57%|██▊  | 5.74G/10.0G [00:30<00:18, 227MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  58%|██▉  | 5.77G/10.0G [00:30<00:18, 227MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  58%|██▉  | 5.80G/10.0G [00:30<00:18, 227MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  58%|██▉  | 5.83G/10.0G [00:30<00:18, 225MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  59%|██▉  | 5.86G/10.0G [00:30<00:18, 225MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  59%|██▉  | 5.89G/10.0G [00:30<00:18, 225MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  59%|██▉  | 5.92G/10.0G [00:30<00:18, 224MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  60%|██▉  | 5.96G/10.0G [00:31<00:18, 224MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  60%|██▉  | 5.99G/10.0G [00:31<00:18, 223MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  60%|███  | 6.02G/10.0G [00:31<00:17, 221MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  61%|███  | 6.05G/10.0G [00:31<00:17, 221MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  61%|███  | 6.08G/10.0G [00:31<00:17, 224MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  61%|███  | 6.11G/10.0G [00:31<00:17, 223MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  61%|███  | 6.14G/10.0G [00:31<00:17, 224MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  62%|███  | 6.18G/10.0G [00:32<00:16, 226MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  62%|███  | 6.21G/10.0G [00:32<00:17, 223MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  62%|███  | 6.24G/10.0G [00:32<00:16, 223MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  63%|███▏ | 6.27G/10.0G [00:32<00:16, 223MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  63%|███▏ | 6.30G/10.0G [00:32<00:16, 222MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  63%|███▏ | 6.33G/10.0G [00:32<00:16, 221MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  64%|███▏ | 6.36G/10.0G [00:32<00:16, 221MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  64%|███▏ | 6.40G/10.0G [00:33<00:16, 217MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  64%|███▏ | 6.43G/10.0G [00:33<00:16, 220MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  65%|███▏ | 6.46G/10.0G [00:33<00:16, 220MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  65%|███▏ | 6.49G/10.0G [00:33<00:15, 222MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  65%|███▎ | 6.52G/10.0G [00:33<00:15, 228MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  66%|███▎ | 6.55G/10.0G [00:33<00:14, 230MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  66%|███▎ | 6.59G/10.0G [00:33<00:14, 231MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  66%|███▎ | 6.62G/10.0G [00:33<00:14, 234MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  66%|███▎ | 6.65G/10.0G [00:34<00:14, 236MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  67%|███▎ | 6.68G/10.0G [00:34<00:13, 242MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  67%|███▎ | 6.71G/10.0G [00:34<00:13, 241MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  67%|███▎ | 6.74G/10.0G [00:34<00:13, 240MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  68%|███▍ | 6.77G/10.0G [00:34<00:13, 240MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  68%|███▍ | 6.81G/10.0G [00:34<00:13, 242MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  68%|███▍ | 6.84G/10.0G [00:34<00:13, 243MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  69%|███▍ | 6.87G/10.0G [00:34<00:12, 243MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  69%|███▍ | 6.90G/10.0G [00:35<00:12, 244MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  69%|███▍ | 6.93G/10.0G [00:35<00:12, 246MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  70%|███▍ | 6.96G/10.0G [00:35<00:14, 206MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  70%|███▍ | 6.99G/10.0G [00:35<00:15, 196MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  70%|███▌ | 7.01G/10.0G [00:35<00:16, 181MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  70%|███▌ | 7.04G/10.0G [00:35<00:17, 173MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  71%|███▌ | 7.06G/10.0G [00:36<00:17, 168MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  71%|███▌ | 7.08G/10.0G [00:36<00:17, 163MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  71%|███▌ | 7.10G/10.0G [00:36<00:18, 160MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  71%|███▌ | 7.12G/10.0G [00:36<00:18, 157MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  71%|███▌ | 7.14G/10.0G [00:36<00:18, 156MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  72%|███▌ | 7.16G/10.0G [00:36<00:18, 156MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  72%|███▌ | 7.18G/10.0G [00:36<00:18, 155MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  72%|███▌ | 7.20G/10.0G [00:37<00:18, 155MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  72%|███▌ | 7.22G/10.0G [00:37<00:17, 156MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  72%|███▌ | 7.25G/10.0G [00:37<00:17, 154MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  73%|███▋ | 7.27G/10.0G [00:37<00:17, 153MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  73%|███▋ | 7.29G/10.0G [00:37<00:17, 152MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  73%|███▋ | 7.31G/10.0G [00:37<00:21, 126MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  73%|███▋ | 7.33G/10.0G [00:37<00:19, 134MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  74%|███▋ | 7.35G/10.0G [00:38<00:21, 122MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  74%|███▋ | 7.37G/10.0G [00:38<00:22, 119MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  74%|███▋ | 7.39G/10.0G [00:38<00:20, 124MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  74%|███▋ | 7.41G/10.0G [00:38<00:20, 123MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  74%|███▋ | 7.43G/10.0G [00:38<00:19, 132MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  75%|███▋ | 7.46G/10.0G [00:38<00:18, 138MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  75%|███▋ | 7.48G/10.0G [00:39<00:17, 143MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  75%|███▋ | 7.50G/10.0G [00:39<00:17, 147MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  75%|███▊ | 7.52G/10.0G [00:39<00:16, 149MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  75%|███▊ | 7.54G/10.0G [00:39<00:16, 150MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  76%|███▊ | 7.56G/10.0G [00:39<00:16, 150MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  76%|███▊ | 7.58G/10.0G [00:39<00:16, 150MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  76%|███▊ | 7.60G/10.0G [00:39<00:16, 149MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  76%|███▊ | 7.62G/10.0G [00:40<00:17, 134MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  76%|███▊ | 7.64G/10.0G [00:40<00:16, 139MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  77%|███▊ | 7.67G/10.0G [00:40<00:16, 143MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  77%|███▊ | 7.69G/10.0G [00:40<00:15, 146MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  77%|███▊ | 7.71G/10.0G [00:40<00:15, 149MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  77%|███▊ | 7.73G/10.0G [00:40<00:15, 150MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  77%|███▊ | 7.75G/10.0G [00:40<00:14, 152MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  78%|███▉ | 7.77G/10.0G [00:41<00:17, 131MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  78%|███▉ | 7.79G/10.0G [00:41<00:16, 136MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  78%|███▉ | 7.81G/10.0G [00:41<00:15, 141MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  78%|███▉ | 7.83G/10.0G [00:41<00:15, 144MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  79%|███▉ | 7.85G/10.0G [00:41<00:14, 146MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  79%|███▉ | 7.87G/10.0G [00:41<00:14, 147MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  79%|███▉ | 7.90G/10.0G [00:41<00:14, 149MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  79%|███▉ | 7.92G/10.0G [00:42<00:13, 152MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  79%|███▉ | 7.94G/10.0G [00:42<00:13, 153MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  80%|███▉ | 7.96G/10.0G [00:42<00:13, 154MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  80%|███▉ | 7.98G/10.0G [00:42<00:13, 154MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  80%|████ | 8.00G/10.0G [00:42<00:12, 155MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  80%|████ | 8.02G/10.0G [00:42<00:12, 155MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  80%|████ | 8.04G/10.0G [00:42<00:12, 153MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  81%|████ | 8.06G/10.0G [00:43<00:12, 152MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  81%|████ | 8.08G/10.0G [00:43<00:12, 151MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  81%|████ | 8.11G/10.0G [00:43<00:12, 152MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  81%|████ | 8.13G/10.0G [00:43<00:12, 152MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  81%|████ | 8.15G/10.0G [00:43<00:12, 152MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  82%|████ | 8.17G/10.0G [00:43<00:12, 151MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  82%|████ | 8.19G/10.0G [00:43<00:12, 150MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  82%|████ | 8.21G/10.0G [00:44<00:12, 148MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  82%|████ | 8.23G/10.0G [00:44<00:12, 147MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  83%|████▏| 8.25G/10.0G [00:44<00:11, 147MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  83%|████▏| 8.27G/10.0G [00:44<00:11, 147MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  83%|████▏| 8.29G/10.0G [00:44<00:11, 147MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  83%|████▏| 8.32G/10.0G [00:44<00:11, 148MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  83%|████▏| 8.34G/10.0G [00:44<00:11, 147MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  84%|████▏| 8.36G/10.0G [00:45<00:11, 147MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  84%|████▏| 8.38G/10.0G [00:45<00:11, 146MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  84%|████▏| 8.40G/10.0G [00:45<00:10, 146MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  84%|████▏| 8.42G/10.0G [00:45<00:10, 147MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  84%|████▏| 8.44G/10.0G [00:45<00:10, 147MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  85%|████▏| 8.46G/10.0G [00:45<00:10, 148MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  85%|████▏| 8.48G/10.0G [00:45<00:10, 148MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  85%|████▎| 8.50G/10.0G [00:46<00:10, 147MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  85%|████▎| 8.52G/10.0G [00:46<00:09, 151MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  85%|████▎| 8.55G/10.0G [00:46<00:09, 153MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  86%|████▎| 8.57G/10.0G [00:46<00:09, 155MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  86%|████▎| 8.59G/10.0G [00:46<00:09, 154MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  86%|████▎| 8.61G/10.0G [00:46<00:09, 154MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  86%|████▎| 8.63G/10.0G [00:46<00:08, 153MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  87%|████▎| 8.65G/10.0G [00:46<00:08, 154MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  87%|████▎| 8.67G/10.0G [00:47<00:08, 153MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  87%|████▎| 8.69G/10.0G [00:47<00:08, 155MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  87%|████▎| 8.71G/10.0G [00:47<00:08, 156MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  87%|████▎| 8.73G/10.0G [00:47<00:08, 155MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  88%|████▍| 8.76G/10.0G [00:47<00:08, 154MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  88%|████▍| 8.78G/10.0G [00:47<00:07, 155MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  88%|████▍| 8.80G/10.0G [00:47<00:07, 155MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  88%|████▍| 8.82G/10.0G [00:48<00:07, 154MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  88%|████▍| 8.84G/10.0G [00:48<00:07, 153MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  89%|████▍| 8.86G/10.0G [00:48<00:07, 152MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  89%|████▍| 8.88G/10.0G [00:48<00:07, 153MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  89%|████▍| 8.90G/10.0G [00:48<00:07, 152MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  89%|████▍| 8.92G/10.0G [00:48<00:07, 152MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  89%|████▍| 8.94G/10.0G [00:48<00:06, 153MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  90%|████▍| 8.97G/10.0G [00:48<00:06, 154MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  90%|████▍| 8.99G/10.0G [00:49<00:06, 155MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  90%|████▌| 9.01G/10.0G [00:49<00:06, 155MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  90%|████▌| 9.03G/10.0G [00:49<00:06, 157MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  90%|████▌| 9.05G/10.0G [00:49<00:06, 156MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  91%|████▌| 9.07G/10.0G [00:49<00:06, 154MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  91%|████▌| 9.09G/10.0G [00:49<00:05, 154MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  91%|████▌| 9.11G/10.0G [00:49<00:05, 152MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  91%|████▌| 9.13G/10.0G [00:50<00:05, 152MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  92%|████▌| 9.15G/10.0G [00:50<00:05, 151MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  92%|████▌| 9.18G/10.0G [00:50<00:05, 151MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  92%|████▌| 9.20G/10.0G [00:50<00:05, 150MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  92%|████▌| 9.22G/10.0G [00:50<00:05, 148MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  92%|████▌| 9.24G/10.0G [00:50<00:05, 148MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  93%|████▋| 9.26G/10.0G [00:50<00:04, 150MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  93%|████▋| 9.28G/10.0G [00:51<00:04, 151MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  93%|████▋| 9.30G/10.0G [00:51<00:04, 152MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  93%|████▋| 9.32G/10.0G [00:51<00:04, 151MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  93%|████▋| 9.34G/10.0G [00:51<00:04, 150MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  94%|████▋| 9.36G/10.0G [00:51<00:04, 129MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  94%|████▋| 9.38G/10.0G [00:51<00:04, 134MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  94%|████▋| 9.41G/10.0G [00:52<00:04, 131MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  94%|████▋| 9.43G/10.0G [00:52<00:04, 137MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  94%|████▋| 9.45G/10.0G [00:52<00:03, 142MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  95%|████▋| 9.47G/10.0G [00:52<00:04, 131MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  95%|████▋| 9.49G/10.0G [00:52<00:04, 105MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  95%|████▊| 9.51G/10.0G [00:52<00:04, 115MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  95%|████▊| 9.53G/10.0G [00:53<00:03, 123MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  96%|████▊| 9.55G/10.0G [00:53<00:03, 131MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  96%|████▊| 9.57G/10.0G [00:53<00:03, 137MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  96%|████▊| 9.59G/10.0G [00:53<00:02, 141MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  96%|████▊| 9.62G/10.0G [00:53<00:02, 143MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  96%|████▊| 9.64G/10.0G [00:53<00:02, 146MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  97%|████▊| 9.66G/10.0G [00:53<00:02, 147MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  97%|████▊| 9.68G/10.0G [00:54<00:02, 149MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  97%|████▊| 9.70G/10.0G [00:54<00:01, 150MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  97%|████▊| 9.72G/10.0G [00:54<00:01, 152MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  97%|████▊| 9.74G/10.0G [00:54<00:01, 154MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  98%|████▉| 9.76G/10.0G [00:54<00:01, 154MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  98%|████▉| 9.78G/10.0G [00:54<00:01, 154MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  98%|████▉| 9.80G/10.0G [00:54<00:01, 152MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  98%|████▉| 9.83G/10.0G [00:54<00:01, 152MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  98%|████▉| 9.85G/10.0G [00:55<00:01, 152MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  99%|████▉| 9.87G/10.0G [00:55<00:00, 151MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  99%|████▉| 9.89G/10.0G [00:55<00:00, 152MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  99%|████▉| 9.91G/10.0G [00:55<00:00, 153MB/s]\u001b[A\npytorch_model-00002-of-00003.bin:  99%|████▉| 9.93G/10.0G [00:55<00:00, 153MB/s]\u001b[A\npytorch_model-00002-of-00003.bin: 100%|████▉| 9.95G/10.0G [00:55<00:00, 153MB/s]\u001b[A\npytorch_model-00002-of-00003.bin: 100%|████▉| 9.97G/10.0G [00:55<00:00, 154MB/s]\u001b[A\npytorch_model-00002-of-00003.bin: 100%|█████| 10.0G/10.0G [00:56<00:00, 178MB/s]\u001b[A\nDownloading shards:  67%|████████████████▋        | 2/3 [01:47<00:54, 54.33s/it]\npytorch_model-00003-of-00003.bin:   0%|             | 0.00/2.71G [00:00<?, ?B/s]\u001b[A\npytorch_model-00003-of-00003.bin:   1%|     | 21.0M/2.71G [00:00<00:20, 132MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:   2%|     | 41.9M/2.71G [00:00<00:18, 142MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:   2%|     | 62.9M/2.71G [00:00<00:18, 145MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:   3%|▏    | 83.9M/2.71G [00:00<00:18, 146MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:   4%|▏     | 105M/2.71G [00:00<00:17, 147MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:   5%|▎     | 126M/2.71G [00:00<00:17, 148MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:   5%|▎     | 147M/2.71G [00:01<00:17, 149MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:   6%|▎     | 168M/2.71G [00:01<00:16, 150MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:   7%|▍     | 189M/2.71G [00:01<00:16, 150MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:   8%|▍     | 210M/2.71G [00:01<00:16, 151MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:   9%|▌     | 231M/2.71G [00:01<00:16, 153MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:   9%|▌     | 252M/2.71G [00:01<00:15, 154MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  10%|▌     | 273M/2.71G [00:01<00:15, 156MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  11%|▋     | 294M/2.71G [00:01<00:15, 156MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  12%|▋     | 315M/2.71G [00:02<00:15, 156MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  12%|▋     | 336M/2.71G [00:02<00:15, 157MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  13%|▊     | 357M/2.71G [00:02<00:14, 158MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  14%|▊     | 377M/2.71G [00:02<00:14, 156MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  15%|▉     | 398M/2.71G [00:02<00:14, 156MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  15%|▉     | 419M/2.71G [00:02<00:14, 156MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  16%|▉     | 440M/2.71G [00:02<00:14, 156MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  17%|█     | 461M/2.71G [00:03<00:14, 156MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  18%|█     | 482M/2.71G [00:03<00:14, 156MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  19%|█     | 503M/2.71G [00:03<00:14, 154MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  19%|█▏    | 524M/2.71G [00:03<00:14, 153MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  20%|█▏    | 545M/2.71G [00:03<00:14, 153MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  21%|█▎    | 566M/2.71G [00:03<00:13, 153MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  22%|█▎    | 587M/2.71G [00:03<00:13, 153MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  22%|█▎    | 608M/2.71G [00:03<00:13, 153MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  23%|█▍    | 629M/2.71G [00:04<00:13, 154MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  24%|█▍    | 650M/2.71G [00:04<00:13, 153MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  25%|█▍    | 671M/2.71G [00:04<00:13, 153MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  26%|█▌    | 692M/2.71G [00:04<00:13, 153MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  26%|█▌    | 713M/2.71G [00:04<00:13, 154MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  27%|█▋    | 734M/2.71G [00:04<00:12, 153MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  28%|█▋    | 755M/2.71G [00:04<00:12, 154MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  29%|█▋    | 776M/2.71G [00:05<00:12, 153MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  29%|█▊    | 797M/2.71G [00:05<00:12, 153MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  30%|█▊    | 818M/2.71G [00:05<00:12, 152MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  31%|█▊    | 839M/2.71G [00:05<00:12, 153MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  32%|█▉    | 860M/2.71G [00:05<00:12, 153MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  33%|█▉    | 881M/2.71G [00:05<00:11, 152MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  33%|█▉    | 902M/2.71G [00:05<00:11, 152MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  34%|██    | 923M/2.71G [00:06<00:11, 152MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  35%|██    | 944M/2.71G [00:06<00:11, 153MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  36%|██▏   | 965M/2.71G [00:06<00:11, 152MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  36%|██▏   | 986M/2.71G [00:06<00:11, 151MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  37%|█▊   | 1.01G/2.71G [00:06<00:11, 150MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  38%|█▉   | 1.03G/2.71G [00:06<00:11, 149MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  39%|█▉   | 1.05G/2.71G [00:06<00:11, 149MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  39%|█▉   | 1.07G/2.71G [00:07<00:11, 148MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  40%|██   | 1.09G/2.71G [00:07<00:10, 147MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  41%|██   | 1.11G/2.71G [00:07<00:10, 147MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  42%|██   | 1.13G/2.71G [00:07<00:10, 148MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  43%|██▏  | 1.15G/2.71G [00:07<00:10, 148MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  43%|██▏  | 1.17G/2.71G [00:07<00:10, 147MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  44%|██▏  | 1.20G/2.71G [00:07<00:10, 148MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  45%|██▏  | 1.22G/2.71G [00:08<00:10, 148MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  46%|██▎  | 1.24G/2.71G [00:08<00:09, 148MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  46%|██▎  | 1.26G/2.71G [00:08<00:09, 148MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  47%|██▎  | 1.28G/2.71G [00:08<00:09, 149MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  48%|██▍  | 1.30G/2.71G [00:08<00:09, 148MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  49%|██▍  | 1.32G/2.71G [00:08<00:09, 148MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  50%|██▍  | 1.34G/2.71G [00:08<00:09, 149MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  50%|██▌  | 1.36G/2.71G [00:09<00:09, 148MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  51%|██▌  | 1.38G/2.71G [00:09<00:08, 148MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  52%|██▌  | 1.41G/2.71G [00:09<00:08, 149MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  53%|██▋  | 1.43G/2.71G [00:09<00:08, 149MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  53%|██▋  | 1.45G/2.71G [00:09<00:08, 149MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  54%|██▋  | 1.47G/2.71G [00:09<00:08, 148MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  55%|██▋  | 1.49G/2.71G [00:09<00:08, 150MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  56%|██▊  | 1.51G/2.71G [00:09<00:07, 152MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  57%|██▊  | 1.53G/2.71G [00:10<00:07, 152MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  57%|██▊  | 1.55G/2.71G [00:10<00:08, 140MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  58%|██▉  | 1.57G/2.71G [00:10<00:09, 125MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  59%|██▉  | 1.59G/2.71G [00:10<00:08, 130MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  60%|██▉  | 1.61G/2.71G [00:10<00:08, 133MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  60%|███  | 1.64G/2.71G [00:10<00:08, 129MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  61%|███  | 1.66G/2.71G [00:11<00:08, 126MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  62%|██▍ | 1.68G/2.71G [00:11<00:11, 90.4MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  63%|███▏ | 1.70G/2.71G [00:11<00:09, 103MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  63%|███▏ | 1.72G/2.71G [00:11<00:08, 114MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  64%|███▏ | 1.74G/2.71G [00:11<00:07, 123MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  65%|███▎ | 1.76G/2.71G [00:12<00:07, 130MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  66%|███▎ | 1.78G/2.71G [00:12<00:07, 128MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  67%|███▎ | 1.80G/2.71G [00:12<00:07, 123MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  67%|███▎ | 1.82G/2.71G [00:12<00:07, 122MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  68%|███▍ | 1.85G/2.71G [00:12<00:06, 124MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  69%|███▍ | 1.87G/2.71G [00:12<00:06, 125MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  70%|███▍ | 1.89G/2.71G [00:13<00:06, 120MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  70%|███▌ | 1.91G/2.71G [00:13<00:07, 108MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  71%|███▌ | 1.93G/2.71G [00:13<00:06, 119MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  72%|███▌ | 1.95G/2.71G [00:13<00:05, 127MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  73%|███▋ | 1.97G/2.71G [00:13<00:05, 133MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  74%|███▋ | 1.99G/2.71G [00:13<00:05, 138MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  74%|███▋ | 2.01G/2.71G [00:14<00:04, 142MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  75%|███▊ | 2.03G/2.71G [00:14<00:04, 145MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  76%|███▊ | 2.06G/2.71G [00:14<00:04, 147MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  77%|███▊ | 2.08G/2.71G [00:14<00:04, 148MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  77%|███▊ | 2.10G/2.71G [00:14<00:04, 151MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  78%|███▉ | 2.12G/2.71G [00:14<00:03, 151MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  79%|███▉ | 2.14G/2.71G [00:14<00:03, 152MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  80%|███▉ | 2.16G/2.71G [00:15<00:03, 152MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  81%|████ | 2.18G/2.71G [00:15<00:03, 153MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  81%|████ | 2.20G/2.71G [00:15<00:03, 152MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  82%|████ | 2.22G/2.71G [00:15<00:03, 151MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  83%|████▏| 2.24G/2.71G [00:15<00:03, 152MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  84%|████▏| 2.26G/2.71G [00:15<00:02, 152MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  84%|████▏| 2.29G/2.71G [00:15<00:02, 151MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  85%|████▎| 2.31G/2.71G [00:16<00:02, 152MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  86%|████▎| 2.33G/2.71G [00:16<00:02, 153MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  87%|████▎| 2.35G/2.71G [00:16<00:02, 153MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  87%|████▎| 2.37G/2.71G [00:16<00:02, 153MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  88%|████▍| 2.39G/2.71G [00:16<00:02, 153MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  89%|████▍| 2.41G/2.71G [00:16<00:01, 152MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  90%|████▍| 2.43G/2.71G [00:16<00:01, 154MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  91%|████▌| 2.45G/2.71G [00:16<00:01, 153MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  91%|████▌| 2.47G/2.71G [00:17<00:01, 154MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  92%|████▌| 2.50G/2.71G [00:17<00:01, 154MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  93%|████▋| 2.52G/2.71G [00:17<00:01, 155MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  94%|████▋| 2.54G/2.71G [00:17<00:01, 155MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  94%|████▋| 2.56G/2.71G [00:17<00:00, 153MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  95%|████▊| 2.58G/2.71G [00:17<00:00, 153MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  96%|████▊| 2.60G/2.71G [00:17<00:00, 152MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  97%|████▊| 2.62G/2.71G [00:18<00:00, 153MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  98%|████▉| 2.64G/2.71G [00:18<00:00, 153MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  98%|████▉| 2.66G/2.71G [00:18<00:00, 153MB/s]\u001b[A\npytorch_model-00003-of-00003.bin:  99%|████▉| 2.68G/2.71G [00:18<00:00, 153MB/s]\u001b[A\npytorch_model-00003-of-00003.bin: 100%|█████| 2.71G/2.71G [00:18<00:00, 145MB/s]\u001b[A\nDownloading shards: 100%|█████████████████████████| 3/3 [02:06<00:00, 42.21s/it]\nSet max length to 16384\nconfig.json: 100%|█████████████████████████████| 508/508 [00:00<00:00, 3.55MB/s]\npytorch_model.bin: 100%|████████████████████| 1.22G/1.22G [00:08<00:00, 147MB/s]\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.embeddings.class_embedding: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.embeddings.patch_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.embeddings.position_embedding.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.pre_layrnorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.pre_layrnorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.0.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.0.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.0.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.0.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.0.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.0.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.0.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.0.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.0.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.0.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.1.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.1.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.1.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.1.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.1.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.1.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.1.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.1.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.1.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.1.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.2.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.2.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.2.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.2.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.2.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.2.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.2.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.2.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.2.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.2.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.3.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.3.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.3.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.3.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.3.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.3.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.3.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.3.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.3.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.3.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.4.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.4.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.4.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.4.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.4.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.4.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.4.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.4.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.4.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.4.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.5.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.5.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.5.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.5.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.5.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.5.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.5.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.5.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.5.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.5.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.6.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.6.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.6.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.6.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.6.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.6.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.6.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.6.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.6.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.6.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.7.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.7.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.7.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.7.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.7.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.7.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.7.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.7.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.7.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.7.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.8.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.8.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.8.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.8.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.8.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.8.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.8.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.8.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.8.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.8.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.9.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.9.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.9.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.9.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.9.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.9.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.9.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.9.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.9.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.9.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.10.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.10.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.10.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.10.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.10.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.10.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.10.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.10.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.10.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.10.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.11.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.11.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.11.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.11.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.11.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.11.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.11.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.11.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.11.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.11.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.12.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.12.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.12.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.12.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.12.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.12.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.12.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.12.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.12.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.12.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.13.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.13.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.13.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.13.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.13.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.13.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.13.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.13.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.13.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.13.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.14.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.14.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.14.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.14.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.14.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.14.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.14.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.14.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.14.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.14.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.15.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.15.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.15.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.15.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.15.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.15.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.15.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.15.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.15.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.15.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.16.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.16.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.16.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.16.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.16.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.16.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.16.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.16.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.16.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.16.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.17.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.17.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.17.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.17.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.17.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.17.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.17.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.17.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.17.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.17.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.18.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.18.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.18.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.18.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.18.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.18.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.18.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.18.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.18.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.18.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.19.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.19.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.19.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.19.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.19.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.19.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.19.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.19.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.19.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.19.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.20.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.20.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.20.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.20.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.20.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.20.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.20.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.20.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.20.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.20.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.21.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.21.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.21.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.21.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.21.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.21.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.21.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.21.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.21.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.21.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.22.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.22.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.22.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.22.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.22.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.22.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.22.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.22.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.22.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.22.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.23.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.23.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.23.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.23.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.23.layer_norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.23.layer_norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.23.mlp.fc1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.23.mlp.fc2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.23.layer_norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.encoder.layers.23.layer_norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.post_layernorm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for vision_model.post_layernorm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\nsam2_hiera_large.pt: 100%|████████████████████| 898M/898M [00:07<00:00, 118MB/s]\n/kaggle/working/GeoPixel/model/sam2/modeling/sam/transformer.py:23: UserWarning: Flash Attention is disabled as it requires a GPU with Ampere (8.0) CUDA capability.\n  OLD_GPU, USE_FLASH_ATTN, MATH_KERNEL_ON = get_sdpa_settings()\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for maskmem_tpos_enc: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for no_mem_embed: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for no_mem_pos_enc: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for no_obj_ptr: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.pos_embed: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.pos_embed_window: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.patch_embed.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.patch_embed.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.0.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.0.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.0.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.0.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.0.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.0.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.0.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.0.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.0.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.0.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.0.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.0.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.1.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.1.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.1.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.1.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.1.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.1.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.1.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.1.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.1.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.1.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.1.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.1.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.2.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.2.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.2.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.2.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.2.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.2.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.2.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.2.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.2.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.2.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.2.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.2.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.2.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.2.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.3.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.3.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.3.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.3.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.3.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.3.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.3.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.3.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.3.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.3.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.3.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.3.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.4.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.4.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.4.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.4.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.4.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.4.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.4.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.4.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.4.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.4.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.4.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.4.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.5.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.5.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.5.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.5.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.5.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.5.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.5.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.5.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.5.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.5.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.5.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.5.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.6.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.6.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.6.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.6.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.6.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.6.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.6.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.6.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.6.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.6.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.6.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.6.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.7.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.7.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.7.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.7.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.7.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.7.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.7.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.7.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.7.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.7.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.7.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.7.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.8.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.8.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.8.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.8.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.8.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.8.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.8.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.8.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.8.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.8.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.8.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.8.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.8.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.8.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.9.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.9.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.9.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.9.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.9.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.9.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.9.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.9.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.9.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.9.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.9.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.9.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.10.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.10.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.10.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.10.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.10.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.10.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.10.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.10.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.10.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.10.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.10.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.10.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.11.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.11.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.11.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.11.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.11.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.11.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.11.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.11.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.11.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.11.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.11.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.11.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.12.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.12.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.12.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.12.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.12.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.12.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.12.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.12.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.12.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.12.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.12.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.12.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.13.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.13.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.13.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.13.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.13.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.13.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.13.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.13.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.13.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.13.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.13.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.13.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.14.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.14.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.14.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.14.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.14.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.14.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.14.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.14.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.14.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.14.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.14.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.14.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.15.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.15.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.15.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.15.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.15.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.15.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.15.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.15.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.15.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.15.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.15.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.15.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.16.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.16.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.16.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.16.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.16.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.16.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.16.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.16.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.16.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.16.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.16.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.16.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.17.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.17.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.17.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.17.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.17.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.17.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.17.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.17.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.17.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.17.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.17.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.17.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.18.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.18.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.18.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.18.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.18.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.18.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.18.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.18.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.18.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.18.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.18.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.18.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.19.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.19.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.19.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.19.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.19.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.19.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.19.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.19.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.19.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.19.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.19.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.19.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.20.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.20.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.20.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.20.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.20.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.20.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.20.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.20.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.20.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.20.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.20.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.20.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.21.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.21.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.21.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.21.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.21.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.21.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.21.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.21.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.21.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.21.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.21.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.21.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.22.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.22.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.22.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.22.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.22.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.22.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.22.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.22.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.22.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.22.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.22.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.22.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.23.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.23.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.23.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.23.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.23.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.23.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.23.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.23.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.23.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.23.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.23.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.23.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.24.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.24.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.24.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.24.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.24.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.24.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.24.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.24.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.24.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.24.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.24.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.24.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.25.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.25.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.25.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.25.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.25.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.25.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.25.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.25.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.25.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.25.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.25.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.25.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.26.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.26.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.26.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.26.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.26.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.26.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.26.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.26.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.26.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.26.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.26.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.26.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.27.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.27.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.27.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.27.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.27.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.27.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.27.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.27.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.27.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.27.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.27.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.27.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.28.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.28.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.28.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.28.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.28.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.28.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.28.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.28.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.28.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.28.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.28.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.28.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.29.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.29.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.29.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.29.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.29.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.29.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.29.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.29.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.29.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.29.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.29.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.29.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.30.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.30.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.30.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.30.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.30.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.30.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.30.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.30.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.30.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.30.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.30.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.30.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.31.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.31.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.31.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.31.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.31.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.31.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.31.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.31.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.31.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.31.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.31.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.31.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.32.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.32.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.32.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.32.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.32.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.32.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.32.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.32.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.32.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.32.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.32.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.32.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.33.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.33.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.33.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.33.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.33.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.33.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.33.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.33.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.33.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.33.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.33.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.33.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.34.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.34.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.34.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.34.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.34.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.34.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.34.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.34.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.34.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.34.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.34.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.34.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.35.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.35.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.35.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.35.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.35.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.35.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.35.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.35.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.35.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.35.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.35.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.35.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.36.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.36.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.36.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.36.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.36.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.36.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.36.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.36.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.36.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.36.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.36.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.36.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.37.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.37.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.37.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.37.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.37.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.37.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.37.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.37.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.37.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.37.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.37.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.37.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.38.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.38.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.38.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.38.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.38.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.38.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.38.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.38.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.38.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.38.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.38.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.38.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.39.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.39.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.39.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.39.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.39.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.39.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.39.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.39.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.39.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.39.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.39.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.39.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.40.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.40.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.40.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.40.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.40.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.40.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.40.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.40.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.40.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.40.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.40.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.40.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.41.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.41.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.41.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.41.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.41.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.41.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.41.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.41.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.41.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.41.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.41.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.41.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.42.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.42.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.42.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.42.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.42.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.42.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.42.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.42.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.42.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.42.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.42.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.42.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.43.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.43.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.43.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.43.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.43.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.43.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.43.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.43.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.43.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.43.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.43.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.43.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.44.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.44.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.44.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.44.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.44.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.44.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.44.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.44.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.44.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.44.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.44.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.44.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.44.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.44.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.45.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.45.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.45.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.45.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.45.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.45.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.45.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.45.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.45.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.45.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.45.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.45.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.46.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.46.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.46.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.46.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.46.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.46.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.46.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.46.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.46.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.46.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.46.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.46.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.47.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.47.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.47.attn.qkv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.47.attn.qkv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.47.attn.proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.47.attn.proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.47.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.47.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.47.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.47.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.47.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.trunk.blocks.47.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.neck.convs.0.conv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.neck.convs.0.conv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.neck.convs.1.conv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.neck.convs.1.conv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.neck.convs.2.conv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.neck.convs.2.conv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.neck.convs.3.conv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for image_encoder.neck.convs.3.conv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for mask_downsample.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for mask_downsample.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.0.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.0.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.0.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.0.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.0.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.0.cross_attn_image.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.0.cross_attn_image.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.0.cross_attn_image.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.0.cross_attn_image.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.0.cross_attn_image.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.0.cross_attn_image.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.0.cross_attn_image.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.0.cross_attn_image.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.0.linear1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.0.linear1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.0.linear2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.0.linear2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.0.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.0.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.0.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.0.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.0.norm3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.0.norm3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.1.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.1.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.1.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.1.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.1.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.1.cross_attn_image.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.1.cross_attn_image.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.1.cross_attn_image.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.1.cross_attn_image.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.1.cross_attn_image.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.1.cross_attn_image.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.1.cross_attn_image.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.1.cross_attn_image.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.1.linear1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.1.linear1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.1.linear2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.1.linear2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.1.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.1.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.1.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.1.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.1.norm3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.1.norm3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.2.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.2.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.2.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.2.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.2.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.2.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.2.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.2.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.2.cross_attn_image.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.2.cross_attn_image.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.2.cross_attn_image.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.2.cross_attn_image.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.2.cross_attn_image.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.2.cross_attn_image.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.2.cross_attn_image.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.2.cross_attn_image.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.2.linear1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.2.linear1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.2.linear2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.2.linear2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.2.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.2.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.2.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.2.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.2.norm3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.2.norm3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.3.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.3.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.3.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.3.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.3.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.3.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.3.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.3.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.3.cross_attn_image.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.3.cross_attn_image.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.3.cross_attn_image.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.3.cross_attn_image.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.3.cross_attn_image.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.3.cross_attn_image.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.3.cross_attn_image.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.3.cross_attn_image.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.3.linear1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.3.linear1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.3.linear2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.3.linear2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.3.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.3.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.3.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.3.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.3.norm3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.layers.3.norm3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_attention.norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.mask_downsampler.encoder.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.mask_downsampler.encoder.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.mask_downsampler.encoder.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.mask_downsampler.encoder.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.mask_downsampler.encoder.3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.mask_downsampler.encoder.3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.mask_downsampler.encoder.4.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.mask_downsampler.encoder.4.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.mask_downsampler.encoder.6.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.mask_downsampler.encoder.6.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.mask_downsampler.encoder.7.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.mask_downsampler.encoder.7.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.mask_downsampler.encoder.9.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.mask_downsampler.encoder.9.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.mask_downsampler.encoder.10.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.mask_downsampler.encoder.10.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.mask_downsampler.encoder.12.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.mask_downsampler.encoder.12.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.pix_feat_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.pix_feat_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.fuser.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.fuser.layers.0.dwconv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.fuser.layers.0.dwconv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.fuser.layers.0.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.fuser.layers.0.norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.fuser.layers.0.pwconv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.fuser.layers.0.pwconv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.fuser.layers.0.pwconv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.fuser.layers.0.pwconv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.fuser.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.fuser.layers.1.dwconv.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.fuser.layers.1.dwconv.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.fuser.layers.1.norm.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.fuser.layers.1.norm.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.fuser.layers.1.pwconv1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.fuser.layers.1.pwconv1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.fuser.layers.1.pwconv2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.fuser.layers.1.pwconv2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for memory_encoder.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_prompt_encoder.point_embeddings.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_prompt_encoder.point_embeddings.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_prompt_encoder.point_embeddings.2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_prompt_encoder.point_embeddings.3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_prompt_encoder.not_a_point_embed.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_prompt_encoder.mask_downscaling.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_prompt_encoder.mask_downscaling.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_prompt_encoder.mask_downscaling.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_prompt_encoder.mask_downscaling.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_prompt_encoder.mask_downscaling.3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_prompt_encoder.mask_downscaling.3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_prompt_encoder.mask_downscaling.4.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_prompt_encoder.mask_downscaling.4.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_prompt_encoder.mask_downscaling.6.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_prompt_encoder.mask_downscaling.6.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_prompt_encoder.no_mask_embed.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.cross_attn_token_to_image.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.norm3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.norm3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.norm4.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.norm4.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.0.cross_attn_image_to_token.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.self_attn.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.self_attn.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.self_attn.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.self_attn.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.self_attn.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.self_attn.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.self_attn.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.self_attn.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.norm1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.norm1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.cross_attn_token_to_image.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.norm2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.norm2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.mlp.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.mlp.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.mlp.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.mlp.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.norm3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.norm3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.norm4.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.norm4.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.layers.1.cross_attn_image_to_token.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.final_attn_token_to_image.q_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.final_attn_token_to_image.q_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.final_attn_token_to_image.k_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.final_attn_token_to_image.k_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.final_attn_token_to_image.v_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.final_attn_token_to_image.v_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.final_attn_token_to_image.out_proj.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.final_attn_token_to_image.out_proj.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.norm_final_attn.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.transformer.norm_final_attn.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.iou_token.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.mask_tokens.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.obj_score_token.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_upscaling.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_upscaling.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_upscaling.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_upscaling.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_upscaling.3.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_upscaling.3.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.conv_s0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.conv_s0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.conv_s1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.conv_s1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_hypernetworks_mlps.0.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_hypernetworks_mlps.0.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_hypernetworks_mlps.0.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_hypernetworks_mlps.0.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_hypernetworks_mlps.0.layers.2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_hypernetworks_mlps.0.layers.2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_hypernetworks_mlps.1.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_hypernetworks_mlps.1.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_hypernetworks_mlps.1.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_hypernetworks_mlps.1.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_hypernetworks_mlps.1.layers.2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_hypernetworks_mlps.1.layers.2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_hypernetworks_mlps.2.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_hypernetworks_mlps.2.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_hypernetworks_mlps.2.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_hypernetworks_mlps.2.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_hypernetworks_mlps.2.layers.2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_hypernetworks_mlps.2.layers.2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_hypernetworks_mlps.3.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_hypernetworks_mlps.3.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_hypernetworks_mlps.3.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_hypernetworks_mlps.3.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_hypernetworks_mlps.3.layers.2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.output_hypernetworks_mlps.3.layers.2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.iou_prediction_head.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.iou_prediction_head.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.iou_prediction_head.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.iou_prediction_head.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.iou_prediction_head.layers.2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.iou_prediction_head.layers.2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.pred_obj_score_head.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.pred_obj_score_head.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.pred_obj_score_head.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.pred_obj_score_head.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.pred_obj_score_head.layers.2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for sam_mask_decoder.pred_obj_score_head.layers.2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for obj_ptr_proj.layers.0.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for obj_ptr_proj.layers.0.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for obj_ptr_proj.layers.1.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for obj_ptr_proj.layers.1.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for obj_ptr_proj.layers.2.weight: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\n/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:2047: UserWarning: for obj_ptr_proj.layers.2.bias: copying from a non-meta parameter in the checkpoint to a meta parameter in the current model, which is a no-op. (Did you mean to pass `assign=True` to assign items in the state dictionary to their corresponding key in the module instead of copying them in place?)\n  warnings.warn(f'for {key}: copying from a non-meta parameter in the checkpoint to a meta '\nLoading checkpoint shards: 100%|██████████████████| 3/3 [01:31<00:00, 30.61s/it]\ngeneration_config.json: 100%|██████████████████| 177/177 [00:00<00:00, 1.62MB/s]\nTraceback (most recent call last):\n  File \"/kaggle/working/GeoPixel/chat.py\", line 123, in <module>\n    main(args)\n  File \"/kaggle/working/GeoPixel/chat.py\", line 61, in main\n    model = model.bfloat16().cuda().eval()\n  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 2054, in cuda\n    return super().cuda(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 915, in cuda\n    return self._apply(lambda t: t.cuda(device))\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 779, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 779, in _apply\n    module._apply(fn)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 779, in _apply\n    module._apply(fn)\n  [Previous line repeated 2 more times]\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 804, in _apply\n    param_applied = fn(param)\n  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 915, in <lambda>\n    return self._apply(lambda t: t.cuda(device))\ntorch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 112.00 MiB. GPU \n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}