{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Grounded SAM Inference",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anarghya15/notebooks/blob/main/notebooks/Grounded_SAM_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install Grounding DINO and Segment Anything Model"
      ],
      "metadata": {
        "id": "Q44_fwHUC0Vj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(\"HOME:\", HOME)\n",
        "%cd {HOME}\n",
        "!git clone https://github.com/IDEA-Research/GroundingDINO.git\n",
        "%cd {HOME}/GroundingDINO\n",
        "!git checkout -q 57535c5a79791cb76e36fdb64975271354f10251\n",
        "!pip install -q -e ."
      ],
      "metadata": {
        "trusted": true,
        "id": "B0KoyUJEC0Vn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "\n",
        "import sys\n",
        "!{sys.executable} -m pip install 'git+https://github.com/facebookresearch/segment-anything.git'\n",
        "!pip uninstall -y supervision\n",
        "!pip install -q supervision==0.6.0"
      ],
      "metadata": {
        "trusted": true,
        "id": "E0cdch5gC0Vp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Grounding DINO Model Weights"
      ],
      "metadata": {
        "id": "TD483-AcC0Vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "GROUNDING_DINO_CONFIG_PATH = os.path.join(HOME, \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\")\n",
        "print(GROUNDING_DINO_CONFIG_PATH, \"; exist:\", os.path.isfile(GROUNDING_DINO_CONFIG_PATH))"
      ],
      "metadata": {
        "trusted": true,
        "id": "fZHBdOPoC0Vq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "!mkdir -p {HOME}/weights\n",
        "%cd {HOME}/weights\n",
        "\n",
        "!wget -q https://github.com/IDEA-Research/GroundingDINO/releases/download/v0.1.0-alpha/groundingdino_swint_ogc.pth\n",
        "\n",
        "GROUNDING_DINO_CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"groundingdino_swint_ogc.pth\")\n",
        "print(GROUNDING_DINO_CHECKPOINT_PATH, \"; exist:\", os.path.isfile(GROUNDING_DINO_CHECKPOINT_PATH))"
      ],
      "metadata": {
        "trusted": true,
        "id": "VP1nmUx6C0Vq"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Segment Anything Model (SAM) Weights"
      ],
      "metadata": {
        "id": "mhOEdQH8C0Vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd {HOME}\n",
        "!mkdir -p {HOME}/weights\n",
        "%cd {HOME}/weights\n",
        "\n",
        "!wget -q https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "\n",
        "SAM_CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n",
        "print(SAM_CHECKPOINT_PATH, \"; exist:\", os.path.isfile(SAM_CHECKPOINT_PATH))"
      ],
      "metadata": {
        "trusted": true,
        "id": "0DfVFS2oC0Vr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Models"
      ],
      "metadata": {
        "id": "16c98AYUC0Vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "%cd {HOME}/GroundingDINO\n",
        "\n",
        "from groundingdino.util.inference import Model\n",
        "\n",
        "grounding_dino_model = Model(model_config_path=GROUNDING_DINO_CONFIG_PATH, model_checkpoint_path=GROUNDING_DINO_CHECKPOINT_PATH)\n",
        "\n",
        "SAM_ENCODER_VERSION = \"vit_h\"\n",
        "\n",
        "from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "sam = sam_model_registry[SAM_ENCODER_VERSION](checkpoint=SAM_CHECKPOINT_PATH).to(device=DEVICE)\n",
        "sam_predictor = SamPredictor(sam)"
      ],
      "metadata": {
        "trusted": true,
        "id": "Np1q2GFLC0Vs"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test"
      ],
      "metadata": {
        "id": "GEpGV4wqC0Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "\n",
        "def detect_and_segment(image_path, classes, BOX_TRESHOLD, TEXT_TRESHOLD):\n",
        "    image = cv2.imread(image_path)\n",
        "\n",
        "    detections = grounding_dino_model.predict_with_classes(\n",
        "        image=image,\n",
        "        classes=enhance_class_name(class_names=classes),\n",
        "        box_threshold=BOX_TRESHOLD,\n",
        "        text_threshold=TEXT_TRESHOLD\n",
        "    )\n",
        "    detections = detections[detections.class_id != None]\n",
        "    detections.mask = segment(\n",
        "        sam_predictor=sam_predictor,\n",
        "        image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB),\n",
        "        xyxy=detections.xyxy\n",
        "    )\n",
        "\n",
        "    labels = [\n",
        "        f\"{classes[class_id]} {confidence:0.2f}\"\n",
        "        for _, _, confidence, class_id, _\n",
        "        in detections]\n",
        "    annotated_image = mask_annotator.annotate(scene=image.copy(), detections=detections)\n",
        "    annotated_image = box_annotator.annotate(scene=annotated_image, detections=detections, labels=labels)\n",
        "\n",
        "    title = \" \".join(set([\n",
        "        classes[class_id]\n",
        "        for class_id\n",
        "        in detections.class_id\n",
        "    ]))\n",
        "\n",
        "    return annotated_image, title"
      ],
      "metadata": {
        "trusted": true,
        "id": "3R_A1WA7C0Vt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classes are generic and threshold is low"
      ],
      "metadata": {
        "id": "MIbK3t-8C0Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE_IMAGE_PATH = f\"{HOME}/data/dog-3.jpeg\"\n",
        "CLASSES = ['car', 'dog', 'person', 'nose', 'chair', 'shoe', 'ear']\n",
        "\n",
        "BOX_TRESHOLD = 0.35\n",
        "TEXT_TRESHOLD = 0.25"
      ],
      "metadata": {
        "trusted": true,
        "id": "p3rXN0weC0Vt"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "annotated_image, title = detect_and_segment(SOURCE_IMAGE_PATH, CLASSES, BOX_TRESHOLD, TEXT_TRESHOLD)\n",
        "%matplotlib inline\n",
        "sv.plot_image(image=annotated_image, title=title, size=(16, 16))"
      ],
      "metadata": {
        "trusted": true,
        "id": "K34BV6jdC0Vu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classes are specific, threshold is low"
      ],
      "metadata": {
        "id": "huGbW1xNC0Vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE_IMAGE_PATH = f\"{HOME}/data/dog-3.jpeg\"\n",
        "CLASSES = ['white car', 'black dog']\n",
        "\n",
        "BOX_TRESHOLD = 0.35\n",
        "TEXT_TRESHOLD = 0.25"
      ],
      "metadata": {
        "trusted": true,
        "id": "5tdp92BpC0Vu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "annotated_image, title = detect_and_segment(SOURCE_IMAGE_PATH, CLASSES, BOX_TRESHOLD, TEXT_TRESHOLD)\n",
        "%matplotlib inline\n",
        "sv.plot_image(image=annotated_image, title=title, size=(16, 16))"
      ],
      "metadata": {
        "trusted": true,
        "id": "4iy9qrk6C0Vu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classes are generic, threshold is high"
      ],
      "metadata": {
        "id": "8WJdxucHC0Vu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE_IMAGE_PATH = f\"{HOME}/data/dog-3.jpeg\"\n",
        "CLASSES = ['car', 'dog', 'person', 'nose', 'chair', 'shoe', 'ear']\n",
        "\n",
        "BOX_TRESHOLD = 0.5\n",
        "TEXT_TRESHOLD = 0.5"
      ],
      "metadata": {
        "trusted": true,
        "id": "L1RGNHMcC0Vu"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "annotated_image, title = detect_and_segment(SOURCE_IMAGE_PATH, CLASSES, BOX_TRESHOLD, TEXT_TRESHOLD)\n",
        "%matplotlib inline\n",
        "sv.plot_image(image=annotated_image, title=title, size=(16, 16))"
      ],
      "metadata": {
        "trusted": true,
        "id": "lkxHwUz-C0Vv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classes are specific threshold is high"
      ],
      "metadata": {
        "id": "R3hJBgy8C0Vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE_IMAGE_PATH = f\"{HOME}/data/dog-3.jpeg\"\n",
        "CLASSES = CLASSES = ['white car', 'black dog']\n",
        "\n",
        "BOX_TRESHOLD = 0.5\n",
        "TEXT_TRESHOLD = 0.5"
      ],
      "metadata": {
        "trusted": true,
        "id": "BR0LHX5GC0Vv"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "annotated_image, title = detect_and_segment(SOURCE_IMAGE_PATH, CLASSES, BOX_TRESHOLD, TEXT_TRESHOLD)\n",
        "%matplotlib inline\n",
        "sv.plot_image(image=annotated_image, title=title, size=(16, 16))"
      ],
      "metadata": {
        "trusted": true,
        "id": "O5mepiPeC0Vv"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}